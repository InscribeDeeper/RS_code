{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:33.924496Z",
     "start_time": "2021-04-20T22:58:33.922496Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T05:59:32.396469Z",
     "start_time": "2021-04-19T05:59:32.311Z"
    }
   },
   "source": [
    "这里要写db的代码去读 数据库中 当天的数据\n",
    "- 如果那个map的key不存在, 则数据库+1\n",
    "- 需要设定当前日期, 然后数据库就会不断 accumulate 到当日\n",
    "- 目前这样的model 适合 长线的数据分析, 不适合RS的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:38:46.555865Z",
     "start_time": "2021-04-22T01:38:44.951503Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\py810\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import datatable\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "dt_folder = \"../data/\"\n",
    "output_folder = \"../processed_data/\"\n",
    "PATH_CLICK = dt_folder+'JD_click_data.csv'\n",
    "PATH_USER = dt_folder+'JD_user_data.csv'\n",
    "PATH_SKU = dt_folder+'JD_sku_data.csv'\n",
    "PATH_ORDER = dt_folder+'JD_order_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T05:34:44.925230Z",
     "start_time": "2020-05-11T05:34:44.923238Z"
    }
   },
   "source": [
    "# DB function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T02:51:06.098611Z",
     "start_time": "2021-04-22T02:51:06.088584Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function to load data : not DB version\n",
    "# =============================================================================\n",
    "\n",
    "def load_daily_data(now= \"2018-03-15\", date_field='order_date', file_path=\"\", Filter = True, from_start = False, output_folder=\"../processed_data/\"):       \n",
    "    '''\n",
    "    order_rec_from_start = load_daily_data(now= \"2018-03-15\", date_field='order_date', file_path=PATH_ORDER, Filter=False, from_start=True, output_folder=output_folder)\n",
    "    click_rec_from_start = load_daily_data(now= \"2018-03-15\", date_field='request_time', file_path=PATH_CLICK, Filter=False, from_start=True, output_folder=output_folder)\n",
    "\n",
    "    load click or order table\n",
    "    input: previous date, now\n",
    "    \n",
    "    it should be the data manipulation on Database\n",
    "    we can write the server side python code to extract similar dataset \n",
    "    but now, I am not that familiar with the pymongo yet. Used pandas to replace the function\n",
    "    增删改查都需要 写对应的function\n",
    "    \n",
    "    具体的数据 filter 可以在这里添加\n",
    "    '''\n",
    "    # to be replaced by DB query\n",
    "    df = datatable.fread(file_path).to_pandas()\n",
    "    \n",
    "    # error checking\n",
    "    if date_field not in df.columns:\n",
    "        print(df.columns)\n",
    "        raise AttributeError(\"data field not in columns. plz check\")\n",
    "        \n",
    "    # time selection\n",
    "    if from_start is False:\n",
    "        predate = (pd.to_datetime(now) + datetime.timedelta(days = -1)).strftime('%Y-%m-%d') # both package works for time manipulation\n",
    "    else:\n",
    "        predate = (pd.to_datetime(now) +  relativedelta(months=-360)).strftime('%Y-%m-%d') # both package works for time manipulation\n",
    "    # df['request_date'] = df['request_time'].apply(lambda x: datetime(x.year, x.month, x.day)) # saved in DB field\n",
    "    df = df[(df[date_field]>predate) & (df[date_field]<=now)]        \n",
    "    \n",
    "    # filter for memory limitation\n",
    "    if Filter:\n",
    "#         user_tab = load_user()\n",
    "#         sku_tab = load_sku()\n",
    "#         user_tab.sample(n=50000, random_state=1).to_csv(output_folder+'target_user.csv')\n",
    "#         sku_tab.sample(n=3000, random_state=1).to_csv(output_folder+'target_sku.csv')\n",
    "        df = df[df['user_ID']!='-'] # delete \"-\" user \n",
    "        target_sku = pd.read_csv(output_folder+'target_sku.csv', index_col=\"sku_ID\")[['type', 'brand_ID', 'attribute1', 'attribute2']]\n",
    "        target_user = pd.read_csv(output_folder+'target_user.csv', index_col=\"user_ID\")[['user_level', 'first_order_month', 'plus', 'gender',\n",
    "                                                                           'age', 'marital_status', 'education', 'city_level', 'purchase_power']]\n",
    "        print(\"target sku rows \", target_sku.shape[0])\n",
    "        print(\"target user rows \", target_user.shape[0])\n",
    "        df = df.merge(target_sku, how='inner', on='sku_ID')\n",
    "        df = df.merge(target_user, how='inner', on='user_ID')\n",
    "\n",
    "\n",
    "    return df \n",
    "\n",
    "def load_user(PATH_USER=PATH_USER, now= \"2018-04\"):\n",
    "    return pd.read_csv(PATH_USER, index_col=\"user_ID\")\n",
    "\n",
    "def load_sku(PATH_SKU=PATH_SKU, now= \"2018-04\"):\n",
    "    return pd.read_csv(PATH_SKU, index_col=\"sku_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T02:48:40.770401Z",
     "start_time": "2021-04-22T02:48:40.225277Z"
    }
   },
   "outputs": [],
   "source": [
    "user_tab = load_user()\n",
    "sku_tab = load_sku()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T03:21:42.970571Z",
     "start_time": "2021-04-22T03:21:42.895990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sku_ID\n",
       "a234e08c57    1_3.0_60.0\n",
       "6449e1fd87    1_2.0_50.0\n",
       "09b70fcd83    2_3.0_70.0\n",
       "2fa77e3b4d         2_-_-\n",
       "08eebd3d4e    2_2.0_50.0\n",
       "                 ...    \n",
       "b3f9712113    2_1.0_90.0\n",
       "600e5918ef    1_4.0_60.0\n",
       "beb742e69e    1_2.0_90.0\n",
       "37165e4030    1_1.0_30.0\n",
       "47895486d3    2_4.0_30.0\n",
       "Length: 77, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sku_tab = load_sku()\n",
    "sku_tab['combined_attr'] = sku_tab['type'].astype(str) + \"_\" +  sku_tab['attribute1'].astype(str) + \"_\" + sku_tab['attribute2'].astype(str) \n",
    "a.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T03:47:14.477816Z",
     "start_time": "2021-04-22T03:47:14.457295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combined_attr\n",
       "1_-_-         [9ec420f27e, 3418d59e22]\n",
       "1_-_100.0     [5c4440c9d2, 7800a54f20]\n",
       "1_-_50.0      [5a745fb2ca, 7732ff947f]\n",
       "1_-_60.0      [027604c03a, ab0c3700ef]\n",
       "1_1.0_-       [82c2ce8c30, fff84ed7ec]\n",
       "                        ...           \n",
       "2_4.0_50.0    [fe33dfddb3, c83994eef3]\n",
       "2_4.0_60.0    [0796a2c783, df12a31fc3]\n",
       "2_4.0_70.0    [723b3c32ff, 4cf90af1c9]\n",
       "2_4.0_80.0    [fa3e664f62, a9f7cb12cd]\n",
       "2_4.0_90.0    [804dc0ce27, baa8387f25]\n",
       "Name: sku_ID, Length: 77, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.groupby(\"combined_attr\")\n",
    "b.apply(lambda x : list(x)[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 在计算 ii 或者uu matrix之前, 可以先用 SVD 进行降维, 以减少 corr计算的负担\n",
    "- 不用PCA, 因为PCA只能作用于方阵\n",
    "SVD = TruncatedSVD(n_components=12, random_state=5)\n",
    "resultant_matrix = SVD.fit_transform(X)\n",
    "resultant_matrix.shape\n",
    "- 继续减少负担的方法\n",
    "    - 先使用物品的隐因子向量进行聚类，海量的物品会减少为少量的类别。然后再逐一计算用户和每个聚类中心的推荐分数，给用户推荐物品就成了给用户推荐物品聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Content-Based Filtering\n",
    "- Content-Based Filtering: Content-Based Filtering is used to produce items recommendation based on items’ characteristics.\n",
    "    \n",
    "- 通过时间 group 一周内 点击量的商品 sum\n",
    "    - def group_by_attrs():\n",
    "        - 返回 attrs:{[item1, item2...]}\n",
    "- 筛选该类型中, 排名最靠前的 10个 作为content based rec\n",
    "    - def get_i_pred_map2\n",
    "        - 找到该商品的 encode, 提取这个encode 对应的10个item\n",
    "        \n",
    "        这里是不是应该把order的信息加进来 作为这个item的信息???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Demographic Filtering\n",
    "- 通过时间 group 一周内 点击量的商品 sum\n",
    "    - def group_by_attrs():\n",
    "        - 返回 attrs:{[item1, item2...]}\n",
    "        - 如果是user的话, 这里的attr 是不一样的\n",
    "- 筛选该类型中, 排名最靠前的 100个 作为content based rec\n",
    "    - def get_u_pred_map3\n",
    "        - 找到该商品的 encode, 提取这个encode 对应的100个item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CF\n",
    "- UI mat应该可以添加更多的东西, \n",
    "    - 计算user 与 user的相似性 之前是通过 click 各种items的数量 的 vector的相似性. 在这个[i1, i2,...]的后面, 还可以加上user 自身的属性, \n",
    "    - 再去计算user的相似性, 并且这个权重要提高\n",
    "    \n",
    "- 同理, 在计算 ii的相似性的时候, 除了点击的user的 vector以外, 还需要 商品自身的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精细筛选 - 参考paper\n",
    "具体的流程为首先通过nlp技术，如word2vec，预训练出所有物品的向量I表示。然后对于每一条用户对物品的点击，将用户的历史点击、历史搜索、地理位置信息等信息经过各自的embedding操作，拼接起来作为输入，经过MLP训练后得到用户的向量表示U，而最终则是通过 softmax 函数来校验U*I的结果是否准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### click sequence -> CBOW embedding -> scoring = rank: item similarity-> KNN ()\n",
    "    - 这里可以结合 scoring 2, scoring 3, 然后在最后一步KNN的时候, 选出均分最高的 items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 怎么获得 iu 两者交互的 embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 最终 根据 order与否, 来训练一个 DL 的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ii CF是什么\n",
    "用这个包DeepCTR, 完成训练. FM 类型的RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T04:00:34.359650Z",
     "start_time": "2021-04-22T04:00:34.342646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_level</th>\n",
       "      <th>first_order_month</th>\n",
       "      <th>plus</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>education</th>\n",
       "      <th>city_level</th>\n",
       "      <th>purchase_power</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000089d6a6</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-08</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000babd1f</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000bc018b</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-06</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>&gt;=56</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000d0e5ab</th>\n",
       "      <td>3</td>\n",
       "      <td>2014-06</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000dce472</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-08</td>\n",
       "      <td>1</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff38690b</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffa1a495</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-09</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>26-35</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffb20ef7</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-11</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>36-45</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffc45330</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-04</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffe74cfb</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-10</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457298 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_level first_order_month  plus gender    age marital_status  \\\n",
       "user_ID                                                                       \n",
       "000089d6a6           1           2017-08     0      F  26-35              S   \n",
       "0000babd1f           1           2018-03     0      U      U              U   \n",
       "0000bc018b           3           2016-06     0      F   >=56              M   \n",
       "0000d0e5ab           3           2014-06     0      M  26-35              M   \n",
       "0000dce472           3           2012-08     1      U      U              U   \n",
       "...                ...               ...   ...    ...    ...            ...   \n",
       "ffff38690b           1           2018-03     0      U      U              U   \n",
       "ffffa1a495           4           2011-09     1      M  26-35              S   \n",
       "ffffb20ef7           3           2017-11     0      M  36-45              M   \n",
       "ffffc45330           1           2016-04     0      F  26-35              M   \n",
       "ffffe74cfb           1           2017-10     0      M  26-35              M   \n",
       "\n",
       "            education  city_level  purchase_power  \n",
       "user_ID                                            \n",
       "000089d6a6          3           4               3  \n",
       "0000babd1f         -1          -1              -1  \n",
       "0000bc018b          3           2               3  \n",
       "0000d0e5ab          3           2               2  \n",
       "0000dce472         -1          -1              -1  \n",
       "...               ...         ...             ...  \n",
       "ffff38690b         -1          -1              -1  \n",
       "ffffa1a495          3           1               2  \n",
       "ffffb20ef7          2           4               2  \n",
       "ffffc45330         -1          -1              -1  \n",
       "ffffe74cfb         -1           3               3  \n",
       "\n",
       "[457298 rows x 9 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T03:32:27.846548Z",
     "start_time": "2021-04-22T03:32:27.833545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>brand_ID</th>\n",
       "      <th>attribute1</th>\n",
       "      <th>attribute2</th>\n",
       "      <th>activate_date</th>\n",
       "      <th>deactivate_date</th>\n",
       "      <th>combined_attr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sku_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a234e08c57</th>\n",
       "      <td>1</td>\n",
       "      <td>c3ab4bf4d9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1_3.0_60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449e1fd87</th>\n",
       "      <td>1</td>\n",
       "      <td>1d8b4b4c63</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1_2.0_50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09b70fcd83</th>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_3.0_70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acad9fed04</th>\n",
       "      <td>2</td>\n",
       "      <td>9b0d3a5fc6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_3.0_70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2fa77e3b4d</th>\n",
       "      <td>2</td>\n",
       "      <td>b681299668</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_-_-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121d8470d2</th>\n",
       "      <td>2</td>\n",
       "      <td>3daeabd2ce</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_3.0_-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e41c62189d</th>\n",
       "      <td>2</td>\n",
       "      <td>8b40ec9ab7</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_-_-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01d16f7678</th>\n",
       "      <td>2</td>\n",
       "      <td>e686890dbc</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_-_-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83fc55d93b</th>\n",
       "      <td>2</td>\n",
       "      <td>9d3465eacc</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_-_-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1b1a4b058</th>\n",
       "      <td>2</td>\n",
       "      <td>65c76167e3</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2_-_-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31868 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            type    brand_ID attribute1 attribute2 activate_date  \\\n",
       "sku_ID                                                             \n",
       "a234e08c57     1  c3ab4bf4d9        3.0       60.0           NaN   \n",
       "6449e1fd87     1  1d8b4b4c63        2.0       50.0           NaN   \n",
       "09b70fcd83     2  eb7d2a675a        3.0       70.0           NaN   \n",
       "acad9fed04     2  9b0d3a5fc6        3.0       70.0           NaN   \n",
       "2fa77e3b4d     2  b681299668          -          -           NaN   \n",
       "...          ...         ...        ...        ...           ...   \n",
       "121d8470d2     2  3daeabd2ce        3.0          -    2018-03-30   \n",
       "e41c62189d     2  8b40ec9ab7          -          -           NaN   \n",
       "01d16f7678     2  e686890dbc          -          -    2018-03-29   \n",
       "83fc55d93b     2  9d3465eacc          -          -    2018-03-29   \n",
       "c1b1a4b058     2  65c76167e3          -          -    2018-03-31   \n",
       "\n",
       "           deactivate_date combined_attr  \n",
       "sku_ID                                    \n",
       "a234e08c57             NaN    1_3.0_60.0  \n",
       "6449e1fd87             NaN    1_2.0_50.0  \n",
       "09b70fcd83             NaN    2_3.0_70.0  \n",
       "acad9fed04             NaN    2_3.0_70.0  \n",
       "2fa77e3b4d             NaN         2_-_-  \n",
       "...                    ...           ...  \n",
       "121d8470d2             NaN       2_3.0_-  \n",
       "e41c62189d             NaN         2_-_-  \n",
       "01d16f7678             NaN         2_-_-  \n",
       "83fc55d93b             NaN         2_-_-  \n",
       "c1b1a4b058             NaN         2_-_-  \n",
       "\n",
       "[31868 rows x 7 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sku_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T03:42:11.035450Z",
     "start_time": "2021-04-22T03:42:11.019447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combined_attr\n",
       "1_3.0_60.0    a234e08c57\n",
       "1_2.0_50.0    6449e1fd87\n",
       "2_3.0_70.0    09b70fcd83\n",
       "2_3.0_70.0    acad9fed04\n",
       "2_-_-         2fa77e3b4d\n",
       "                 ...    \n",
       "2_3.0_-       121d8470d2\n",
       "2_-_-         e41c62189d\n",
       "2_-_-         01d16f7678\n",
       "2_-_-         83fc55d93b\n",
       "2_-_-         c1b1a4b058\n",
       "Name: sku_ID, Length: 31868, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sku_tab.reset_index().set_index(\"combined_attr\")['sku_ID']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T04:07:34.051881Z",
     "start_time": "2021-04-22T04:07:34.029876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_ID</th>\n",
       "      <th>user_ID</th>\n",
       "      <th>request_time</th>\n",
       "      <th>channel</th>\n",
       "      <th>type</th>\n",
       "      <th>brand_ID</th>\n",
       "      <th>attribute1</th>\n",
       "      <th>attribute2</th>\n",
       "      <th>user_level</th>\n",
       "      <th>first_order_month</th>\n",
       "      <th>plus</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>education</th>\n",
       "      <th>city_level</th>\n",
       "      <th>purchase_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 22:10:51</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 13:50:40</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 13:53:56</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 13:51:59</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 13:53:24</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39542</th>\n",
       "      <td>d5753123ae</td>\n",
       "      <td>68a822e5ea</td>\n",
       "      <td>2018-03-14 19:06:57</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>556646a4ed</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39543</th>\n",
       "      <td>d5753123ae</td>\n",
       "      <td>68a822e5ea</td>\n",
       "      <td>2018-03-14 19:00:20</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>556646a4ed</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39544</th>\n",
       "      <td>d5753123ae</td>\n",
       "      <td>68a822e5ea</td>\n",
       "      <td>2018-03-14 18:58:19</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>556646a4ed</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39545</th>\n",
       "      <td>274d201816</td>\n",
       "      <td>f087de6d21</td>\n",
       "      <td>2018-03-14 13:08:18</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>e0c4997859</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>16-25</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39546</th>\n",
       "      <td>274d201816</td>\n",
       "      <td>0ce5847c1a</td>\n",
       "      <td>2018-03-14 12:49:48</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>e0c4997859</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>16-25</td>\n",
       "      <td>S</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39547 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sku_ID     user_ID         request_time channel  type    brand_ID  \\\n",
       "0      09b70fcd83  2791ec4485  2018-03-01 22:10:51  wechat     2  eb7d2a675a   \n",
       "1      09b70fcd83  2791ec4485  2018-03-01 13:50:40  wechat     2  eb7d2a675a   \n",
       "2      09b70fcd83  2791ec4485  2018-03-01 13:53:56  wechat     2  eb7d2a675a   \n",
       "3      09b70fcd83  2791ec4485  2018-03-01 13:51:59  wechat     2  eb7d2a675a   \n",
       "4      09b70fcd83  2791ec4485  2018-03-01 13:53:24  wechat     2  eb7d2a675a   \n",
       "...           ...         ...                  ...     ...   ...         ...   \n",
       "39542  d5753123ae  68a822e5ea  2018-03-14 19:06:57     app     2  556646a4ed   \n",
       "39543  d5753123ae  68a822e5ea  2018-03-14 19:00:20     app     2  556646a4ed   \n",
       "39544  d5753123ae  68a822e5ea  2018-03-14 18:58:19     app     2  556646a4ed   \n",
       "39545  274d201816  f087de6d21  2018-03-14 13:08:18     app     2  e0c4997859   \n",
       "39546  274d201816  0ce5847c1a  2018-03-14 12:49:48     app     2  e0c4997859   \n",
       "\n",
       "      attribute1 attribute2  user_level first_order_month  plus gender    age  \\\n",
       "0            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "1            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "2            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "3            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "4            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "...          ...        ...         ...               ...   ...    ...    ...   \n",
       "39542        4.0      100.0           2           2017-09     0      F  26-35   \n",
       "39543        4.0      100.0           2           2017-09     0      F  26-35   \n",
       "39544        4.0      100.0           2           2017-09     0      F  26-35   \n",
       "39545          -          -           1           2017-09     0      F  16-25   \n",
       "39546          -          -           2           2017-10     0      F  16-25   \n",
       "\n",
       "      marital_status  education  city_level  purchase_power  \n",
       "0                  M          2           3               3  \n",
       "1                  M          2           3               3  \n",
       "2                  M          2           3               3  \n",
       "3                  M          2           3               3  \n",
       "4                  M          2           3               3  \n",
       "...              ...        ...         ...             ...  \n",
       "39542              U         -1           3              -1  \n",
       "39543              U         -1           3              -1  \n",
       "39544              U         -1           3              -1  \n",
       "39545              S          3           3               3  \n",
       "39546              S         -1           3               4  \n",
       "\n",
       "[39547 rows x 17 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T03:12:30.961219Z",
     "start_time": "2021-04-22T03:12:30.938214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>brand_ID</th>\n",
       "      <th>attribute1</th>\n",
       "      <th>attribute2</th>\n",
       "      <th>activate_date</th>\n",
       "      <th>deactivate_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sku_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a234e08c57</th>\n",
       "      <td>1</td>\n",
       "      <td>c3ab4bf4d9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449e1fd87</th>\n",
       "      <td>1</td>\n",
       "      <td>1d8b4b4c63</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09b70fcd83</th>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acad9fed04</th>\n",
       "      <td>2</td>\n",
       "      <td>9b0d3a5fc6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2fa77e3b4d</th>\n",
       "      <td>2</td>\n",
       "      <td>b681299668</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5325af6848</th>\n",
       "      <td>2</td>\n",
       "      <td>3daeabd2ce</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddb9f1d372</th>\n",
       "      <td>2</td>\n",
       "      <td>4dd88e766d</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59dbfc95e9</th>\n",
       "      <td>2</td>\n",
       "      <td>77265a3888</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74cc3cc7a4</th>\n",
       "      <td>2</td>\n",
       "      <td>1c3c5a5a3c</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1b1a4b058</th>\n",
       "      <td>2</td>\n",
       "      <td>65c76167e3</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6038 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            type    brand_ID attribute1 attribute2 activate_date  \\\n",
       "sku_ID                                                             \n",
       "a234e08c57     1  c3ab4bf4d9        3.0       60.0           NaN   \n",
       "6449e1fd87     1  1d8b4b4c63        2.0       50.0           NaN   \n",
       "09b70fcd83     2  eb7d2a675a        3.0       70.0           NaN   \n",
       "acad9fed04     2  9b0d3a5fc6        3.0       70.0           NaN   \n",
       "2fa77e3b4d     2  b681299668          -          -           NaN   \n",
       "...          ...         ...        ...        ...           ...   \n",
       "5325af6848     2  3daeabd2ce        3.0          -    2018-03-30   \n",
       "ddb9f1d372     2  4dd88e766d          -          -    2018-03-31   \n",
       "59dbfc95e9     2  77265a3888        2.0       50.0           NaN   \n",
       "74cc3cc7a4     2  1c3c5a5a3c        3.0       50.0    2018-03-31   \n",
       "c1b1a4b058     2  65c76167e3          -          -    2018-03-31   \n",
       "\n",
       "           deactivate_date  \n",
       "sku_ID                      \n",
       "a234e08c57             NaN  \n",
       "6449e1fd87             NaN  \n",
       "09b70fcd83             NaN  \n",
       "acad9fed04             NaN  \n",
       "2fa77e3b4d             NaN  \n",
       "...                    ...  \n",
       "5325af6848             NaN  \n",
       "ddb9f1d372             NaN  \n",
       "59dbfc95e9             NaN  \n",
       "74cc3cc7a4             NaN  \n",
       "c1b1a4b058             NaN  \n",
       "\n",
       "[6038 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sku_tab.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T02:57:25.683028Z",
     "start_time": "2021-04-22T02:57:25.680027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000020D7F67CA08>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 怎么根据属性 去grouping?\n",
    "user_tab.groupby('user_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:38:50.595438Z",
     "start_time": "2021-04-22T01:38:50.588437Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# utils\n",
    "# =============================================================================\n",
    "\n",
    "def ts_str2sec(format_time):\n",
    "    '''\n",
    "    input: format_time = \"2018-03-01 13:21:04\"\n",
    "    output: timeStamp = 1381419600\n",
    "    '''\n",
    "    ts = time.strptime(format_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return time.mktime(ts)  \n",
    "\n",
    "def ts_sec2str(timeStamp):\n",
    "    '''\n",
    "    input:  timeStamp = 1381419600\n",
    "    output: format_time = \"2018-03-01 13:21:04\"\n",
    "    '''\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(timeStamp))\n",
    "\n",
    "def ts_attrs_add(df, ts_col = 'request_time'):\n",
    "    # num_samples=10000\n",
    "    # ######### generate request_date\n",
    "    # df = read_csv(PATH_CLICK, nrows=num_samples)\n",
    "    # ts_col = 'request_time'\n",
    "    df[ts_col] = to_datetime(df[ts_col])\n",
    "    df[ts_col+'_sec'] = df[ts_col].astype(str).progress_apply(ts_str2sec)\n",
    "    \n",
    "    # For visulization\n",
    "    df['hour'] = df[ts_col].dt.hour\n",
    "    df['day'] = df[ts_col].dt.day\n",
    "    df['month'] = df[ts_col].dt.month\n",
    "    df['year'] = df[ts_col].dt.year\n",
    "    df['daysinmonth'] = df[ts_col].dt.daysinmonth\n",
    "    df['dayofyear'] = df[ts_col].dt.dayofyear \n",
    "    \n",
    "    # year-month-day\n",
    "    df[ts_col[0:-4]+'date'] = df[ts_col].dt.date\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate user-item matrix by day\n",
    "- it can be done by query as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:38:52.202376Z",
     "start_time": "2021-04-22T01:38:52.197374Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_user_item_matrix(daily_dt, based='click'):\n",
    "    \"\"\"\n",
    "    @Params:\n",
    "        daily_dt: the user_ID and sku_ID clicking or ordering record\n",
    "        based: \"click\" if input the click table, \"order\" if input the order table\n",
    "    \"\"\"\n",
    "    dt = daily_dt\n",
    "    \n",
    "    if based == \"click\":\n",
    "        dt = dt.groupby(by=['user_ID','sku_ID']).count().reset_index() # grouping by day\n",
    "        dt = dt.pivot_table(index='user_ID', columns='sku_ID', values='request_time') # panel data \n",
    "    elif based == \"order\":\n",
    "        dt = dt.groupby(by=['user_ID','sku_ID'])['quantity'].sum().reset_index() # grouping by day\n",
    "        dt = dt.pivot_table(index='user_ID', columns='sku_ID', values='quantity') # panel data \n",
    "        \n",
    "    dt = dt.fillna(0).astype(\"int16\") # NaN value imputing # large upcast\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:24:27.328896Z",
     "start_time": "2021-04-19T18:24:27.326895Z"
    }
   },
   "source": [
    "## update user_item_matrix_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:38:54.183822Z",
     "start_time": "2021-04-22T01:38:54.179820Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_user_item_matrix_by_day(user_item_from_start, user_item_daily):\n",
    "    '''\n",
    "    it will update the loaded ui_mtx by daily data\n",
    "    the records effective days are not considered yet\n",
    "    the bought records are not considered yet (which should have less impact if the user will not buy the same items again)\n",
    "    \n",
    "    @Params:\n",
    "        user_item_from_start = big matrix on DB\n",
    "        user_item_daily = daily computed matrix \n",
    "    '''\n",
    "    \n",
    "    # user_item_from_start.sum()['1c1453e829']\n",
    "    # incremental_tab.sum()['1c1453e829']\n",
    "    # updated.sum()['1c1453e829']\n",
    "\n",
    "    incremental_tab = (user_item_from_start*0) # preserve the index and column but with zero value\n",
    "    incremental_tab.update(user_item_daily, overwrite=True) # update the target part\n",
    "    updated = (user_item_from_start + incremental_tab)\n",
    "    return updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measure user similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:38:54.956996Z",
     "start_time": "2021-04-22T01:38:54.950995Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_item_sim(updated_user_item_matrix, Filter=True):\n",
    "    \"\"\"\n",
    "    it take the ui_mtx, filtering, then calc the pearson correlation coefficient as similarity score\n",
    "    \"\"\"\n",
    "    if Filter:\n",
    "        user_sets = updated_user_item_matrix.sum(axis=1).nlargest(1000).index # only consider top 1000 active user to get the item similarity\n",
    "        updated_user_item_matrix = updated_user_item_matrix.loc[user_sets]\n",
    "    \n",
    "    mat = updated_user_item_matrix\n",
    "    assert mat.columns.names[0] == \"sku_ID\"\n",
    "    return mat.corr()\n",
    "    \n",
    "def get_user_sim(updated_user_item_matrix, Filter=True):\n",
    "    \"\"\"\n",
    "    it take the iu_mtx, filtering, then calc the pearson correlation coefficient as similarity score\n",
    "    \"\"\"    \n",
    "    if Filter:\n",
    "        item_sets = updated_user_item_matrix.sum(axis=0).nlargest(100).index # only consider top 100 popular items to get the user similarity\n",
    "        updated_user_item_matrix = updated_user_item_matrix[item_sets]\n",
    "        \n",
    "    mat = updated_user_item_matrix.T\n",
    "    assert mat.columns.names[0] == \"user_ID\"\n",
    "    return mat.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate prediction matrix for one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:38:56.760422Z",
     "start_time": "2021-04-22T01:38:56.756422Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_topK_idx(x, topK):\n",
    "    if isinstance(x, pd.Series):\n",
    "        x = x.tolist()\n",
    "    #  or isinstance(x, np.ndarray)\n",
    "    idx = np.argsort(x)[::-1][0:topK]\n",
    "    return idx\n",
    "\n",
    "# def _get_topK_for_each(user, updated_user_item_matrix, user_cor, topNeighbor = 1000, rs_topItem = 10):\n",
    "#     '''\n",
    "#     # test code for one user\n",
    "#     rs_topItem = 10\n",
    "#     topNeighbor = 1000\n",
    "#     user = updated_user_item_matrix.index[0]\n",
    "#     updated_user_item_matrix = update_user_item_matrix_by_day(user_item_from_start, user_item_daily)\n",
    "#     user_cor = get_user_sim(updated_user_item_matrix)\n",
    "\n",
    "#     get_topK_for_each(user, updated_user_item_matrix=updated_user_item_matrix, user_cor=user_cor, topNeighbor = 1000, rs_topItem = 10)\n",
    "    \n",
    "#     '''\n",
    "#     sim_mat = user_cor.loc[user].nlargest(topNeighbor) # select one column and get the most similar neighbors\n",
    "#     topN_sim_user = sim_mat.index\n",
    "#     r_bp = updated_user_item_matrix.loc[topN_sim_user]\n",
    "#     r_delta = r_bp - r_bp.mean(axis=1).values.reshape(-1,1)\n",
    "#     # updated_user_item_matrix.loc[user].mean() + np.average(b_hat, weights=sim_mat, axis=0)\n",
    "#     pred_for_user = updated_user_item_matrix.loc[user].mean() + np.dot(sim_mat.values, r_delta.values) / sim_mat.values.sum()\n",
    "    \n",
    "#     ## get top K\n",
    "#     topK_idx = np.argsort(pred_for_user)[::-1][0:rs_topItem]\n",
    "#     topK_ID = updated_user_item_matrix.columns[topK_idx].tolist() # extract ID from input matrix\n",
    "#     return topK_ID\n",
    "\n",
    "# # rs_pred = list(map(lambda user: get_topK_for_each(user, updated_user_item_matrix=updated_user_item_matrix, user_cor=user_cor, topNeighbor = 1000, rs_topItem = 10), updated_user_item_matrix.index.values))\n",
    "# user_cor = get_user_sim(updated_user_item_matrix)\n",
    "# rs_pred = {}\n",
    "# for x_user in tqdm(updated_user_item_matrix.index.values):\n",
    "#     rs_pred[x_user] = get_topK_for_each(x_user, updated_user_item_matrix=updated_user_item_matrix, user_cor=user_cor, topNeighbor = 1000, rs_topItem = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:38:57.725146Z",
     "start_time": "2021-04-22T01:38:57.717145Z"
    }
   },
   "outputs": [],
   "source": [
    "# import swifter # only works on linux system\n",
    "\n",
    "def get_score(x, updated_user_item_matrix, users_sim):\n",
    "    \"\"\"\n",
    "    ref: 7-Recommenders.pdf page 25 -> \n",
    "    pred(a, p) = ra_hat + similarity_ratio * (neighbors_score - neighbors_avg_score)\n",
    "    \"\"\"\n",
    "    r_bp = updated_user_item_matrix.loc[x.values[0]] # retrieve similar use index by x.values[0] : nTop_user x all_items\n",
    "    r_delta = r_bp - r_bp.mean(axis=1).values.reshape(-1,1) # calc average click and delta for b item : nTop_user x all_items\n",
    "    res = updated_user_item_matrix.loc[x.name].mean() + np.dot(users_sim.loc[x.name], r_delta.values) / np.sum(users_sim.loc[x.name])# : nTop_user x all_items dot nTop_user_similarity_score\n",
    "    return res\n",
    "\n",
    "def get_u_pred_map(updated_user_item_matrix, topNeighbor=100, rs_topItem=10):\n",
    "    \"\"\"\n",
    "    it is really time consuming when calc the score for each user\n",
    "    \n",
    "    user based CF:  \n",
    "        build a map that: given an user, return recommendate items    \n",
    "        Pearson (correlation)-based similarity\n",
    "    \n",
    "    @Params:\n",
    "        topNeighbor: how many similar items would be considered on KNN\n",
    "        rs_topItem: how many similar items would be saved on the hashmap for recommendation\n",
    "    Return:\n",
    "        the user to item hashmap\n",
    "    \"\"\"    \n",
    "    user_cor = get_user_sim(updated_user_item_matrix, Filter=True)\n",
    "    topN_sim_users = user_cor.apply(lambda x: x.index[get_topK_idx(x, topNeighbor)].tolist(), axis=1).to_frame()\n",
    "    users_sim = user_cor.apply(lambda x: x.values[get_topK_idx(x, topNeighbor)], axis=1) \n",
    "    # users_sim = user_cor.apply(lambda x: pd.Series(x.values[get_topK_idx(x, topNeighbor)]), axis=1) \n",
    "    score = topN_sim_users.progress_apply(lambda x: get_score(x, updated_user_item_matrix=updated_user_item_matrix, users_sim=users_sim), axis=1) # x 是一个user_ID\n",
    "    pred_utab = score.apply(lambda x: updated_user_item_matrix.columns[get_topK_idx(x, rs_topItem)].tolist()) # get top items index by \"np.argsort(x)[::-1][0:rs_topItem]\"\n",
    "    return pred_utab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item based recommendation\n",
    "- cosine-based similarity\n",
    "- Minimum number of users for each item-item pair: 5 (see below for explanation)\n",
    "- Number of similar items stored: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:39:00.112718Z",
     "start_time": "2021-04-22T01:38:59.980688Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def get_i_pred_map(updated_user_item_matrix, topNeighbor=100, rs_topItem=10):\n",
    "    \"\"\"\n",
    "    item based KNN:\n",
    "        Cosine-based similarity\n",
    "        build a map that: given an item, return recommendate items\n",
    "    \n",
    "    Parameters:\n",
    "        topNeighbor: how many similar items would be considered on KNN\n",
    "        rs_topItem: how many similar items would be saved on the hashmap for recommendation\n",
    "    Return:\n",
    "        the item to item hashmap\n",
    "    \"\"\"    \n",
    "    updated_iu_mtx = updated_user_item_matrix.T\n",
    "    idx = updated_iu_mtx.index\n",
    "    model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_jobs=-1).fit(updated_iu_mtx)\n",
    "    # topNeighbor = 10\n",
    "    distances, indices = model_knn.kneighbors(updated_iu_mtx, n_neighbors=topNeighbor) # compute Knearest for each item\n",
    "    d = dict(zip(idx, map(lambda x: idx[x], indices))) # reverse index\n",
    "\n",
    "    # get item similarity, which can also be used to make recommendation when input a item\n",
    "    item_KNN_prediction = pd.DataFrame.from_dict(d, orient='index', columns=[\"top\" + str(x) for x in range(1,topNeighbor+1)]) \n",
    "    pred_itab = item_KNN_prediction\n",
    "\n",
    "    return pred_itab\n",
    "\n",
    "\n",
    "\n",
    "def get_u_pred_map2(updated_user_item_matrix, topNeighbor=100, rs_topItem=10):\n",
    "    \"\"\"\n",
    "    item based CF:  \n",
    "        build a map that: given an user, return recommendate items    \n",
    "        Cosine-based similarity\n",
    "        based on similar items from KNN, this function take the average score of these items as the score for target item\n",
    "    Parameters:\n",
    "        topNeighbor: how many similar items would be considered on KNN\n",
    "        rs_topItem: how many similar items would be saved on the hashmap for recommendation\n",
    "    Return:\n",
    "        the user to item hashmap\n",
    "    \"\"\"    \n",
    "    updated_iu_mtx = updated_user_item_matrix.T\n",
    "    item_KNN_prediction = get_i_pred_map(updated_user_item_matrix, topNeighbor=100, rs_topItem=10) # cosine similarity rather than pearson\n",
    "    # get neighbors items average clicks  \n",
    "        ## 1.找到最相似的10个item 保存到KNN_prediction tab中;  2. 找到那10个item在总表中每个users的得分, 计算其均值, 输出一个行向量 (每一个element是一个user的对这个item的 得分均值)\n",
    "    score = item_KNN_prediction.apply(lambda x: updated_iu_mtx.loc[x.tolist()].mean(), axis=1)\n",
    "    # get top K based on neighbors items average clicks  \n",
    "        ## 3.对每一个user, 找到 均值得分 排名最高的20个item\n",
    "    # rs_topItem = 20 \n",
    "        ## np.argsort(x.values) 必须用values, 因为如果x是pd.series, 则会根据 key 的字符串 去排序\n",
    "    pred_utab = score.T.apply(lambda x:x.index[get_topK_idx(x, rs_topItem)].tolist(), axis=1) # 取出来的是一个series, 所以需要用index, 而不是columns (虽然x是一行)\n",
    "    return pred_utab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.246302Z",
     "start_time": "2021-04-20T22:58:34.520Z"
    }
   },
   "outputs": [],
   "source": [
    "# offline 每天训练一个knn, 并且找到最近的item, 基于这些item的平均click, 去预测这个用户明天的推荐清单b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main func -  CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start-up \n",
    "- only run once for system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:39:03.825638Z",
     "start_time": "2021-04-22T01:39:03.821583Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 0:  fake initialization to a time t0 \"2018-03-13\"\n",
    "test = False\n",
    "if test:\n",
    "    rec_from_start = load_daily_data(now= \"2018-03-13\", date_field='request_time', file_path=PATH_CLICK, Filter=True, from_start=True, output_folder=output_folder)\n",
    "    user_item_from_start = generate_user_item_matrix(rec_from_start, based='click')\n",
    "         ## should be replaced with DB QUERY ## factorization could be applied to save storage ## some rollback mechanism should implemented on DB level\n",
    "    user_item_from_start.to_csv(output_folder+'CF_click/today/user_item_from_start.csv') \n",
    "    pd.read_csv(output_folder+'CF_click/today/user_item_from_start.csv',index_col = \"user_ID\").to_csv(output_folder+'CF_click/predate/user_item_from_start.csv') # copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily offline process - server side\n",
    "- DB version should be used to replace this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T23:01:14.644989Z",
     "start_time": "2021-04-20T23:01:00.648869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target sku rows  3000\n",
      "target user rows  50000\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: load saved user_item_from_start from previous day.\n",
    "    ## should be replaced with DB QUERY\n",
    "user_item_from_start = pd.read_csv(output_folder+'CF_click/predate/user_item_from_start.csv', index_col='user_ID')\n",
    "\n",
    "# STEP 2: generate daily incremental data offline for today\n",
    "rec_daily = load_daily_data(now= \"2018-03-13\", date_field='request_time', file_path=PATH_CLICK, output_folder=output_folder)\n",
    "user_item_daily = generate_user_item_matrix(rec_daily, based='click')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T23:01:22.292494Z",
     "start_time": "2021-04-20T23:01:16.791102Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 3: update user_item_from_start and saved into DB\n",
    "updated_user_item_matrix = update_user_item_matrix_by_day(user_item_from_start, user_item_daily)\n",
    "updated_user_item_matrix.to_csv(output_folder+'CF_click/today/user_item_from_start.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T23:48:37.112700Z",
     "start_time": "2021-04-20T23:48:30.214076Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 4: compute recommendation item matrix for each user\n",
    "rs_topItem = 10\n",
    "topNeighbor = 100\n",
    "# u_rs_pred = get_u_pred_map(updated_user_item_matrix, topNeighbor, rs_topItem)\n",
    "i_rs_pred = get_i_pred_map(updated_user_item_matrix, topNeighbor, rs_topItem)\n",
    "u_rs_pred2 = get_u_pred_map2(updated_user_item_matrix, topNeighbor, rs_topItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T23:49:17.600156Z",
     "start_time": "2021-04-20T23:49:15.460168Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for one day\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: saved rs_pred_json into DB & DB copy finish step\n",
    "    # it should be used on online env \n",
    "rs_pred_json = json.dumps(u_rs_pred2.to_dict())\n",
    "f = open(output_folder+'CF_click/online_rs_dict.json', 'w')\n",
    "f.write(rs_pred_json)\n",
    "f.close()\n",
    "\n",
    "pd.read_csv(output_folder+'CF_click/today/user_item_from_start.csv',index_col = \"user_ID\").to_csv(output_folder+'CF_click/predate/user_item_from_start.csv') # copy\n",
    "print(\"Done for one day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: make recommendation\n",
    "\n",
    "# def make_recommendations():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:40:51.592627Z",
     "start_time": "2021-04-22T01:40:32.373652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target sku rows  3000\n",
      "target user rows  50000\n"
     ]
    }
   ],
   "source": [
    "click_rec_from_start = load_daily_data(now= \"2018-03-15\", date_field='request_time', file_path=PATH_CLICK, Filter=True, from_start=True, output_folder=output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T02:37:37.807797Z",
     "start_time": "2021-04-22T02:37:37.803796Z"
    }
   },
   "outputs": [],
   "source": [
    "dt = click_rec_from_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:39:48.143691Z",
     "start_time": "2021-04-22T01:39:47.451535Z"
    }
   },
   "outputs": [],
   "source": [
    "order_rec_from_start = load_daily_data(now= \"2018-03-15\", date_field='order_date', file_path=PATH_ORDER, Filter=False, from_start=True, output_folder=output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:49:49.584578Z",
     "start_time": "2021-04-22T01:49:49.544569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sku_ID\n",
       "000aa92b82      9\n",
       "000d4af39d      1\n",
       "000dc27e13      1\n",
       "00104dbcd7     47\n",
       "001b448016      2\n",
       "             ... \n",
       "ffea16c182      5\n",
       "fff33f1633      2\n",
       "fff4328ec0      1\n",
       "fffe1bd280    402\n",
       "fffe6eb4df      3\n",
       "Name: quantity, Length: 6411, dtype: int32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = order_rec_from_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T02:37:41.760949Z",
     "start_time": "2021-04-22T02:37:41.737943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_ID</th>\n",
       "      <th>user_ID</th>\n",
       "      <th>request_time</th>\n",
       "      <th>channel</th>\n",
       "      <th>type</th>\n",
       "      <th>brand_ID</th>\n",
       "      <th>attribute1</th>\n",
       "      <th>attribute2</th>\n",
       "      <th>user_level</th>\n",
       "      <th>first_order_month</th>\n",
       "      <th>plus</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>education</th>\n",
       "      <th>city_level</th>\n",
       "      <th>purchase_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 22:10:51</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 13:50:40</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 13:53:56</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 13:51:59</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09b70fcd83</td>\n",
       "      <td>2791ec4485</td>\n",
       "      <td>2018-03-01 13:53:24</td>\n",
       "      <td>wechat</td>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39542</th>\n",
       "      <td>d5753123ae</td>\n",
       "      <td>68a822e5ea</td>\n",
       "      <td>2018-03-14 19:06:57</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>556646a4ed</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39543</th>\n",
       "      <td>d5753123ae</td>\n",
       "      <td>68a822e5ea</td>\n",
       "      <td>2018-03-14 19:00:20</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>556646a4ed</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39544</th>\n",
       "      <td>d5753123ae</td>\n",
       "      <td>68a822e5ea</td>\n",
       "      <td>2018-03-14 18:58:19</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>556646a4ed</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39545</th>\n",
       "      <td>274d201816</td>\n",
       "      <td>f087de6d21</td>\n",
       "      <td>2018-03-14 13:08:18</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>e0c4997859</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-09</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>16-25</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39546</th>\n",
       "      <td>274d201816</td>\n",
       "      <td>0ce5847c1a</td>\n",
       "      <td>2018-03-14 12:49:48</td>\n",
       "      <td>app</td>\n",
       "      <td>2</td>\n",
       "      <td>e0c4997859</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>16-25</td>\n",
       "      <td>S</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39547 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sku_ID     user_ID         request_time channel  type    brand_ID  \\\n",
       "0      09b70fcd83  2791ec4485  2018-03-01 22:10:51  wechat     2  eb7d2a675a   \n",
       "1      09b70fcd83  2791ec4485  2018-03-01 13:50:40  wechat     2  eb7d2a675a   \n",
       "2      09b70fcd83  2791ec4485  2018-03-01 13:53:56  wechat     2  eb7d2a675a   \n",
       "3      09b70fcd83  2791ec4485  2018-03-01 13:51:59  wechat     2  eb7d2a675a   \n",
       "4      09b70fcd83  2791ec4485  2018-03-01 13:53:24  wechat     2  eb7d2a675a   \n",
       "...           ...         ...                  ...     ...   ...         ...   \n",
       "39542  d5753123ae  68a822e5ea  2018-03-14 19:06:57     app     2  556646a4ed   \n",
       "39543  d5753123ae  68a822e5ea  2018-03-14 19:00:20     app     2  556646a4ed   \n",
       "39544  d5753123ae  68a822e5ea  2018-03-14 18:58:19     app     2  556646a4ed   \n",
       "39545  274d201816  f087de6d21  2018-03-14 13:08:18     app     2  e0c4997859   \n",
       "39546  274d201816  0ce5847c1a  2018-03-14 12:49:48     app     2  e0c4997859   \n",
       "\n",
       "      attribute1 attribute2  user_level first_order_month  plus gender    age  \\\n",
       "0            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "1            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "2            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "3            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "4            3.0       70.0           1           2018-02     0      F  26-35   \n",
       "...          ...        ...         ...               ...   ...    ...    ...   \n",
       "39542        4.0      100.0           2           2017-09     0      F  26-35   \n",
       "39543        4.0      100.0           2           2017-09     0      F  26-35   \n",
       "39544        4.0      100.0           2           2017-09     0      F  26-35   \n",
       "39545          -          -           1           2017-09     0      F  16-25   \n",
       "39546          -          -           2           2017-10     0      F  16-25   \n",
       "\n",
       "      marital_status  education  city_level  purchase_power  \n",
       "0                  M          2           3               3  \n",
       "1                  M          2           3               3  \n",
       "2                  M          2           3               3  \n",
       "3                  M          2           3               3  \n",
       "4                  M          2           3               3  \n",
       "...              ...        ...         ...             ...  \n",
       "39542              U         -1           3              -1  \n",
       "39543              U         -1           3              -1  \n",
       "39544              U         -1           3              -1  \n",
       "39545              S          3           3               3  \n",
       "39546              S         -1           3               4  \n",
       "\n",
       "[39547 rows x 17 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T02:37:39.318793Z",
     "start_time": "2021-04-22T02:37:39.241776Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: quantity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c6eb46277403>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sku_ID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"quantity\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1611\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m             )\n\u001b[1;32m-> 1613\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_gotitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Column not found: {key}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Column not found: quantity'"
     ]
    }
   ],
   "source": [
    "dt.groupby(\"sku_ID\")[\"quantity\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:49:56.737497Z",
     "start_time": "2021-04-22T01:49:56.715492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_ID</th>\n",
       "      <th>user_ID</th>\n",
       "      <th>sku_ID</th>\n",
       "      <th>order_date</th>\n",
       "      <th>order_time</th>\n",
       "      <th>quantity</th>\n",
       "      <th>type</th>\n",
       "      <th>promise</th>\n",
       "      <th>original_unit_price</th>\n",
       "      <th>final_unit_price</th>\n",
       "      <th>direct_discount_per_unit</th>\n",
       "      <th>quantity_discount_per_unit</th>\n",
       "      <th>bundle_discount_per_unit</th>\n",
       "      <th>coupon_discount_per_unit</th>\n",
       "      <th>gift_item</th>\n",
       "      <th>dc_ori</th>\n",
       "      <th>dc_des</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0cf5cc6db</td>\n",
       "      <td>0abe9ef2ce</td>\n",
       "      <td>581d5b54c1</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2018-03-01 17:14:25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>89.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7444318d01</td>\n",
       "      <td>33a9e56257</td>\n",
       "      <td>067b673f2b</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2018-03-01 11:10:40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>99.9</td>\n",
       "      <td>53.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f973b01694</td>\n",
       "      <td>4ea3cf408f</td>\n",
       "      <td>623d0a582a</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2018-03-01 09:13:26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>78.0</td>\n",
       "      <td>58.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8c1cec8d4b</td>\n",
       "      <td>b87cb736cb</td>\n",
       "      <td>fc5289b139</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2018-03-01 21:29:50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d43a33c38a</td>\n",
       "      <td>4829223b6f</td>\n",
       "      <td>623d0a582a</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2018-03-01 19:13:37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278189</th>\n",
       "      <td>3a03c8dfc5</td>\n",
       "      <td>403543b55f</td>\n",
       "      <td>81e57cbc50</td>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>2018-03-15 11:12:11.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>148.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278190</th>\n",
       "      <td>49bf7a68ec</td>\n",
       "      <td>985c1d24b3</td>\n",
       "      <td>7e4cb4952a</td>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>2018-03-15 15:57:11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>59.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278191</th>\n",
       "      <td>6c432b12e4</td>\n",
       "      <td>efa03b649c</td>\n",
       "      <td>9e452b5aee</td>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>2018-03-15 09:25:09.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278192</th>\n",
       "      <td>ce26ac8d0f</td>\n",
       "      <td>3ec047b8b1</td>\n",
       "      <td>3632a990f7</td>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>2018-03-15 20:45:41.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278193</th>\n",
       "      <td>05312edee8</td>\n",
       "      <td>ba18edc3e0</td>\n",
       "      <td>ce35b83746</td>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>2018-03-15 19:30:28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>68.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278194 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          order_ID     user_ID      sku_ID  order_date             order_time  \\\n",
       "0       d0cf5cc6db  0abe9ef2ce  581d5b54c1  2018-03-01  2018-03-01 17:14:25.0   \n",
       "1       7444318d01  33a9e56257  067b673f2b  2018-03-01  2018-03-01 11:10:40.0   \n",
       "2       f973b01694  4ea3cf408f  623d0a582a  2018-03-01  2018-03-01 09:13:26.0   \n",
       "3       8c1cec8d4b  b87cb736cb  fc5289b139  2018-03-01  2018-03-01 21:29:50.0   \n",
       "4       d43a33c38a  4829223b6f  623d0a582a  2018-03-01  2018-03-01 19:13:37.0   \n",
       "...            ...         ...         ...         ...                    ...   \n",
       "278189  3a03c8dfc5  403543b55f  81e57cbc50  2018-03-15  2018-03-15 11:12:11.0   \n",
       "278190  49bf7a68ec  985c1d24b3  7e4cb4952a  2018-03-15  2018-03-15 15:57:11.0   \n",
       "278191  6c432b12e4  efa03b649c  9e452b5aee  2018-03-15  2018-03-15 09:25:09.0   \n",
       "278192  ce26ac8d0f  3ec047b8b1  3632a990f7  2018-03-15  2018-03-15 20:45:41.0   \n",
       "278193  05312edee8  ba18edc3e0  ce35b83746  2018-03-15  2018-03-15 19:30:28.0   \n",
       "\n",
       "        quantity  type promise  original_unit_price  final_unit_price  \\\n",
       "0              1     2       -                 89.0              79.0   \n",
       "1              1     1       2                 99.9              53.9   \n",
       "2              1     1       2                 78.0              58.5   \n",
       "3              1     1       2                 61.0              35.0   \n",
       "4              1     1       1                 78.0              53.0   \n",
       "...          ...   ...     ...                  ...               ...   \n",
       "278189        15     1       2                148.0             135.0   \n",
       "278190         1     1       2                 59.0              49.0   \n",
       "278191         1     1       2                 89.0              89.0   \n",
       "278192         2     2       -                  0.0               0.0   \n",
       "278193         1     2       -                 68.0              68.0   \n",
       "\n",
       "        direct_discount_per_unit  quantity_discount_per_unit  \\\n",
       "0                            0.0                        10.0   \n",
       "1                            5.0                        41.0   \n",
       "2                           19.5                         0.0   \n",
       "3                            0.0                        26.0   \n",
       "4                           19.0                         0.0   \n",
       "...                          ...                         ...   \n",
       "278189                      13.0                         0.0   \n",
       "278190                      10.0                         0.0   \n",
       "278191                       0.0                         0.0   \n",
       "278192                       0.0                         0.0   \n",
       "278193                       0.0                         0.0   \n",
       "\n",
       "        bundle_discount_per_unit  coupon_discount_per_unit  gift_item  dc_ori  \\\n",
       "0                            0.0                       0.0      False       4   \n",
       "1                            0.0                       0.0      False      28   \n",
       "2                            0.0                       0.0      False      28   \n",
       "3                            0.0                       0.0      False       4   \n",
       "4                            0.0                       6.0      False       3   \n",
       "...                          ...                       ...        ...     ...   \n",
       "278189                       0.0                       0.0      False       2   \n",
       "278190                       0.0                       0.0      False       2   \n",
       "278191                       0.0                       0.0      False       2   \n",
       "278192                       0.0                       0.0       True       4   \n",
       "278193                       0.0                       0.0      False       4   \n",
       "\n",
       "        dc_des  \n",
       "0           28  \n",
       "1           28  \n",
       "2           28  \n",
       "3           28  \n",
       "4           16  \n",
       "...        ...  \n",
       "278189       2  \n",
       "278190       2  \n",
       "278191       2  \n",
       "278192      28  \n",
       "278193      28  \n",
       "\n",
       "[278194 rows x 17 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:41:19.364020Z",
     "start_time": "2021-04-22T01:41:19.351017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>brand_ID</th>\n",
       "      <th>attribute1</th>\n",
       "      <th>attribute2</th>\n",
       "      <th>activate_date</th>\n",
       "      <th>deactivate_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sku_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a234e08c57</th>\n",
       "      <td>1</td>\n",
       "      <td>c3ab4bf4d9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449e1fd87</th>\n",
       "      <td>1</td>\n",
       "      <td>1d8b4b4c63</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09b70fcd83</th>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acad9fed04</th>\n",
       "      <td>2</td>\n",
       "      <td>9b0d3a5fc6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2fa77e3b4d</th>\n",
       "      <td>2</td>\n",
       "      <td>b681299668</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121d8470d2</th>\n",
       "      <td>2</td>\n",
       "      <td>3daeabd2ce</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e41c62189d</th>\n",
       "      <td>2</td>\n",
       "      <td>8b40ec9ab7</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01d16f7678</th>\n",
       "      <td>2</td>\n",
       "      <td>e686890dbc</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83fc55d93b</th>\n",
       "      <td>2</td>\n",
       "      <td>9d3465eacc</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1b1a4b058</th>\n",
       "      <td>2</td>\n",
       "      <td>65c76167e3</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31868 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            type    brand_ID attribute1 attribute2 activate_date  \\\n",
       "sku_ID                                                             \n",
       "a234e08c57     1  c3ab4bf4d9        3.0       60.0           NaN   \n",
       "6449e1fd87     1  1d8b4b4c63        2.0       50.0           NaN   \n",
       "09b70fcd83     2  eb7d2a675a        3.0       70.0           NaN   \n",
       "acad9fed04     2  9b0d3a5fc6        3.0       70.0           NaN   \n",
       "2fa77e3b4d     2  b681299668          -          -           NaN   \n",
       "...          ...         ...        ...        ...           ...   \n",
       "121d8470d2     2  3daeabd2ce        3.0          -    2018-03-30   \n",
       "e41c62189d     2  8b40ec9ab7          -          -           NaN   \n",
       "01d16f7678     2  e686890dbc          -          -    2018-03-29   \n",
       "83fc55d93b     2  9d3465eacc          -          -    2018-03-29   \n",
       "c1b1a4b058     2  65c76167e3          -          -    2018-03-31   \n",
       "\n",
       "           deactivate_date  \n",
       "sku_ID                      \n",
       "a234e08c57             NaN  \n",
       "6449e1fd87             NaN  \n",
       "09b70fcd83             NaN  \n",
       "acad9fed04             NaN  \n",
       "2fa77e3b4d             NaN  \n",
       "...                    ...  \n",
       "121d8470d2             NaN  \n",
       "e41c62189d             NaN  \n",
       "01d16f7678             NaN  \n",
       "83fc55d93b             NaN  \n",
       "c1b1a4b058             NaN  \n",
       "\n",
       "[31868 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sku_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:41:18.055703Z",
     "start_time": "2021-04-22T01:41:17.527584Z"
    }
   },
   "outputs": [],
   "source": [
    "user_tab = load_user()\n",
    "sku_tab = load_sku()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:30:34.398003Z",
     "start_time": "2021-04-22T01:30:34.364995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_level</th>\n",
       "      <th>first_order_month</th>\n",
       "      <th>plus</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>education</th>\n",
       "      <th>city_level</th>\n",
       "      <th>purchase_power</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000089d6a6</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-08</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000babd1f</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000bc018b</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-06</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>&gt;=56</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000d0e5ab</th>\n",
       "      <td>3</td>\n",
       "      <td>2014-06</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000dce472</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-08</td>\n",
       "      <td>1</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff38690b</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03</td>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffa1a495</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-09</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>26-35</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffb20ef7</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-11</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>36-45</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffc45330</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-04</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffe74cfb</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-10</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>26-35</td>\n",
       "      <td>M</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457298 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_level first_order_month  plus gender    age marital_status  \\\n",
       "user_ID                                                                       \n",
       "000089d6a6           1           2017-08     0      F  26-35              S   \n",
       "0000babd1f           1           2018-03     0      U      U              U   \n",
       "0000bc018b           3           2016-06     0      F   >=56              M   \n",
       "0000d0e5ab           3           2014-06     0      M  26-35              M   \n",
       "0000dce472           3           2012-08     1      U      U              U   \n",
       "...                ...               ...   ...    ...    ...            ...   \n",
       "ffff38690b           1           2018-03     0      U      U              U   \n",
       "ffffa1a495           4           2011-09     1      M  26-35              S   \n",
       "ffffb20ef7           3           2017-11     0      M  36-45              M   \n",
       "ffffc45330           1           2016-04     0      F  26-35              M   \n",
       "ffffe74cfb           1           2017-10     0      M  26-35              M   \n",
       "\n",
       "            education  city_level  purchase_power  \n",
       "user_ID                                            \n",
       "000089d6a6          3           4               3  \n",
       "0000babd1f         -1          -1              -1  \n",
       "0000bc018b          3           2               3  \n",
       "0000d0e5ab          3           2               2  \n",
       "0000dce472         -1          -1              -1  \n",
       "...               ...         ...             ...  \n",
       "ffff38690b         -1          -1              -1  \n",
       "ffffa1a495          3           1               2  \n",
       "ffffb20ef7          2           4               2  \n",
       "ffffc45330         -1          -1              -1  \n",
       "ffffe74cfb         -1           3               3  \n",
       "\n",
       "[457298 rows x 9 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T01:30:39.782303Z",
     "start_time": "2021-04-22T01:30:39.770300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>brand_ID</th>\n",
       "      <th>attribute1</th>\n",
       "      <th>attribute2</th>\n",
       "      <th>activate_date</th>\n",
       "      <th>deactivate_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sku_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a234e08c57</th>\n",
       "      <td>1</td>\n",
       "      <td>c3ab4bf4d9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449e1fd87</th>\n",
       "      <td>1</td>\n",
       "      <td>1d8b4b4c63</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09b70fcd83</th>\n",
       "      <td>2</td>\n",
       "      <td>eb7d2a675a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acad9fed04</th>\n",
       "      <td>2</td>\n",
       "      <td>9b0d3a5fc6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2fa77e3b4d</th>\n",
       "      <td>2</td>\n",
       "      <td>b681299668</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121d8470d2</th>\n",
       "      <td>2</td>\n",
       "      <td>3daeabd2ce</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e41c62189d</th>\n",
       "      <td>2</td>\n",
       "      <td>8b40ec9ab7</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01d16f7678</th>\n",
       "      <td>2</td>\n",
       "      <td>e686890dbc</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83fc55d93b</th>\n",
       "      <td>2</td>\n",
       "      <td>9d3465eacc</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1b1a4b058</th>\n",
       "      <td>2</td>\n",
       "      <td>65c76167e3</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31868 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            type    brand_ID attribute1 attribute2 activate_date  \\\n",
       "sku_ID                                                             \n",
       "a234e08c57     1  c3ab4bf4d9        3.0       60.0           NaN   \n",
       "6449e1fd87     1  1d8b4b4c63        2.0       50.0           NaN   \n",
       "09b70fcd83     2  eb7d2a675a        3.0       70.0           NaN   \n",
       "acad9fed04     2  9b0d3a5fc6        3.0       70.0           NaN   \n",
       "2fa77e3b4d     2  b681299668          -          -           NaN   \n",
       "...          ...         ...        ...        ...           ...   \n",
       "121d8470d2     2  3daeabd2ce        3.0          -    2018-03-30   \n",
       "e41c62189d     2  8b40ec9ab7          -          -           NaN   \n",
       "01d16f7678     2  e686890dbc          -          -    2018-03-29   \n",
       "83fc55d93b     2  9d3465eacc          -          -    2018-03-29   \n",
       "c1b1a4b058     2  65c76167e3          -          -    2018-03-31   \n",
       "\n",
       "           deactivate_date  \n",
       "sku_ID                      \n",
       "a234e08c57             NaN  \n",
       "6449e1fd87             NaN  \n",
       "09b70fcd83             NaN  \n",
       "acad9fed04             NaN  \n",
       "2fa77e3b4d             NaN  \n",
       "...                    ...  \n",
       "121d8470d2             NaN  \n",
       "e41c62189d             NaN  \n",
       "01d16f7678             NaN  \n",
       "83fc55d93b             NaN  \n",
       "c1b1a4b058             NaN  \n",
       "\n",
       "[31868 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sku_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_i_pred_map(updated_user_item_matrix, topNeighbor, rs_topItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main func - for order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start-up \n",
    "- only run once for system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.253303Z",
     "start_time": "2021-04-20T22:58:34.996Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 0:  fake initialization to a time t0 \"2018-03-13\"\n",
    "rec_from_start = load_daily_data(now= \"2018-03-13\", date_field='request_time', file_path=PATH_CLICK, from_start=True,  output_folder=output_folder)\n",
    "user_item_from_start = generate_user_item_matrix(rec_from_start, based='click')\n",
    "     ## should be replaced with DB QUERY ## factorization could be applied to save storage ## some rollback mechanism should implemented on DB level\n",
    "user_item_from_start.to_csv(output_folder+'CF_click/today/user_item_from_start.csv') \n",
    "pd.read_csv(output_folder+'CF_click/today/user_item_from_start.csv').to_csv(output_folder+'CF_click/predate/user_item_from_start.csv') # copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.254302Z",
     "start_time": "2021-04-20T22:58:34.999Z"
    }
   },
   "outputs": [],
   "source": [
    "## fake initialization to a time t0\n",
    "rec_from_start = load_daily_data(now= \"2018-03-13\", date_field='order_date', file_path=PATH_ORDER, from_start = True,  output_folder=output_folder)\n",
    "# user_item_from_start = generate_user_item_order_matrix(rec_from_start)\n",
    "\n",
    "## generate daily \n",
    "# rec_daily = load_daily_data(now= \"2018-03-13\", date_field='order_date', file_path=PATH_ORDER)\n",
    "# user_item_daily = generate_user_item_matrix(rec_daily, based='order')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily offline process - server side\n",
    "- DB version should be used to replace this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load click and user table \n",
    "- Time attrs added\n",
    "- label encoding for \"sku_ID\", \"user_ID\", \"order_ID\" and perserve the original one for final evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.255303Z",
     "start_time": "2021-04-20T22:58:35.125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_click_order(click_cols = ['user_ID', 'sku_ID',  'day', 'month', 'year', 'hour','request_time_sec'],\n",
    "                     order_cols = ['user_ID', 'sku_ID',  'day', 'month', 'year', 'hour', 'order_ID', 'order_time_sec'],\n",
    "                     sku_cols = ['sku_ID', 'type', 'brand_ID'],\n",
    "                     sort = ['user_ID', 'request_time_sec'], num_samples=None, need_encode=True, file_version='test'):\n",
    "    '''\n",
    "    \n",
    "    1. 读取4个表\n",
    "    2. 添加时间属性3列 (day month year) etc\n",
    "    3. label encoding (user_ID, sku_ID). origin ID 保存在user_table 和sku_table\n",
    "    4. outer join table (因为暂时还不确定是在一天内. 如果确定在一天内 request链接 并下单, 则用left join)\n",
    "        - 例如:  左侧为空, 右侧有的, 就是可能在前些天 有request, 然后过了几天才下单. 这个问题需要通过 request_time_sec 来做差解决\n",
    "    5. 返回df (做EDA的数据已经另存, 此处不需要返回 order_table 和 click_table)\n",
    "    \n",
    "    click_table.columns = ['sku_ID', 'user_ID', 'request_time', 'channel', 'request_time_sec',\n",
    "           'hour', 'day', 'month', 'year', 'daysinmonth', 'dayofyear',\n",
    "           'request_date']     \n",
    "\n",
    "    order_table.columns = ['order_ID', 'user_ID', 'sku_ID', 'order_date', 'order_time', 'quantity',\n",
    "           'type', 'promise', 'original_unit_price', 'final_unit_price',\n",
    "           'direct_discount_per_unit', 'quantity_discount_per_unit',\n",
    "           'bundle_discount_per_unit', 'coupon_discount_per_unit', 'gift_item',\n",
    "           'dc_ori', 'dc_des', 'order_time_sec', 'hour', 'day', 'month', 'year',\n",
    "           'daysinmonth', 'dayofyear']\n",
    "    \n",
    "    sku_table.columns = ['sku_ID', 'type', 'brand_ID', 'attribute1', 'attribute2',\n",
    "           'activate_date', 'deactivate_date', 'origin_sku_ID']\n",
    "    '''\n",
    "#     click_table = load_dataset.load_click(sort=None)[cols1]\n",
    "#     order_table = load_dataset.load_order()[cols2]\n",
    "#     click_table = load_click(sort=None, num_samples=num_samples)[click_cols1] # 已去除 \"-\" 用户\n",
    "#     order_table = load_order(num_samples=num_samples)[order_cols2]\n",
    "\n",
    "\n",
    "\n",
    "    click_table = load_click(sort=None, num_samples=num_samples)[click_cols] # 已去除 \"-\" 用户\n",
    "    order_table = load_order(num_samples=num_samples)[order_cols]\n",
    "    if (need_encode== False): # 先转换label encoding, 再join效率高\n",
    "        \n",
    "            # 这一步可以在数据库内完成, 而且可以连接 a.day = b.day-1\n",
    "        df = click_table.merge(order_table, how='left',\n",
    "                          left_on = ['user_ID', 'sku_ID', 'day', 'month', 'year'],\n",
    "                          right_on = ['user_ID', 'sku_ID', 'day', 'month', 'year']) # 这里应该是left, 找到所有同一天 点击+下单 的 用户+sku+时间\n",
    "\n",
    "        df['if_order'] =  1*(~df.order_ID.isnull()) \n",
    "        \n",
    "    elif (need_encode== True):\n",
    "        user_table = pd.read_csv(PATH_USER, nrows=num_samples)\n",
    "        sku_table = pd.read_csv(PATH_SKU, nrows=num_samples)\n",
    "        \n",
    "        ## fit_transform \n",
    "        sku_le = preprocessing.LabelEncoder().fit(pd.concat([click_table['sku_ID'],order_table['sku_ID'],sku_table[\"sku_ID\"]], axis=0).astype(str))\n",
    "        click_table['sku_ID'] = sku_le.transform(click_table['sku_ID'])\n",
    "        order_table['sku_ID'] = sku_le.transform(order_table['sku_ID'])\n",
    "        sku_table['origin_sku_ID'] = sku_table['sku_ID'] # 保留原有ID\n",
    "        sku_table['sku_ID'] = sku_le.transform(sku_table['origin_sku_ID']) # 更新 label\n",
    "        \n",
    "        \n",
    "        user_le = preprocessing.LabelEncoder().fit(pd.concat([click_table['user_ID'],order_table['user_ID'], user_table['user_ID']], axis=0).astype(str))\n",
    "        click_table['user_ID'] = user_le.transform(click_table['user_ID'])\n",
    "        order_table['user_ID'] = user_le.transform(order_table['user_ID'])\n",
    "        user_table['origin_user_ID'] = user_table['user_ID'] # 保留原有ID   \n",
    "        user_table['user_ID'] = user_le.transform(user_table['origin_user_ID']) # 更新label\n",
    "        \n",
    "        \n",
    "        order_le = preprocessing.LabelEncoder().fit(order_table['order_ID'].astype(str))\n",
    "        order_table['order_ID'] = order_le.transform(order_table['order_ID'].astype(str))\n",
    "        \n",
    "        # 这一步可以在数据库内完成, 而且可以连接 a.day = b.day-1\n",
    "        df = click_table.merge(order_table, how='outer',\n",
    "                          left_on = ['user_ID', 'sku_ID', 'day', 'month', 'year'],\n",
    "                          right_on = ['user_ID', 'sku_ID', 'day', 'month', 'year']) \n",
    "        \n",
    "        ## Brand_ID\n",
    "        df = df.merge(sku_table[sku_cols],how='left',left_on =['sku_ID'], right_on=['sku_ID'])\n",
    "        \n",
    "        df['if_order'] =  1*(~df.order_ID.isnull()) \n",
    "        \n",
    "        \n",
    "        if file_version != 'test':\n",
    "            df.to_csv(output_folder+\"all_dt_\"+file_version+\".csv\")\n",
    "            user_table.to_csv(output_folder+\"user_table.csv\")\n",
    "            sku_table.to_csv(output_folder+\"sku_table.csv\")\n",
    "    \n",
    "\n",
    "    if sort:\n",
    "        df.sort_values(sort, inplace=True)\n",
    "    return df\n",
    "            \n",
    "\n",
    "\n",
    "# only execute once\n",
    "a= load_click_order(num_samples=None, need_encode=True, file_version='v1')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.256303Z",
     "start_time": "2021-04-20T22:58:35.128Z"
    }
   },
   "outputs": [],
   "source": [
    "output_folder = \"../processed_data/\"\n",
    "num_samples = 100000\n",
    "file_version = \"v1\"\n",
    "df = pd.read_csv(output_folder+'all_dt_'+file_version+'.csv',nrows=num_samples, index_col=0) # click_user_table\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.257303Z",
     "start_time": "2021-04-20T22:58:35.131Z"
    }
   },
   "outputs": [],
   "source": [
    "# a= load_click_order(num_samples=10000000, load=False, file_version='v1') # load generated dataset\n",
    "# a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Combined user features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> Key Assumption: </font>\n",
    "- In this period, the preference of user will not change\n",
    "- The order only happen in the same day as request\n",
    "    - It could be released later if we want to generate more samples\n",
    "- In all the n-grams samples (user request sku list), choose window_size = 5, predict the middle one which is ordered sku_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.369328Z",
     "start_time": "2021-04-20T22:58:35.353325Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't save it before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\py810\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-16dd837b370e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0muser_table_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_encode_user_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0muser_table_encoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;31m# sku_map = pd.read_pickle('sku_map.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# encode user_table\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from pandas import read_csv, datetime, to_datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "import time\n",
    "\n",
    "\n",
    "# encode_user_table 生成\n",
    "def load_encode_user_table(load=True,nrows=None):\n",
    "    if load == False:\n",
    "        user_table = pd.read_csv('user_table.csv', nrows=nrows)\n",
    "        tmp = user_table[['user_level', 'gender','education', 'city_level', 'purchase_power','marital_status','age']].astype(str).progress_apply(lambda x: \"__\".join(list(x)), axis=1)\n",
    "        user_table_encoded = user_table[['user_ID','origin_user_ID']]\n",
    "        user_table_encoded['user_encode'] = tmp\n",
    "        # user_map_dict = user_table_encoded.set_index('user_ID').T.to_dict() # 这个转换过程特别慢. 但是后续合并很快\n",
    "        user_table_encoded.to_pickle('user_table_encoded.pkl')\n",
    "    else:\n",
    "        try:\n",
    "            user_table_encoded = pd.read_pickle('user_table_encoded.pkl')\n",
    "\n",
    "        except:\n",
    "            print(\"Didn't save it before\")\n",
    "            return None\n",
    "    return user_table_encoded   \n",
    "\n",
    "user_table_encoded = load_encode_user_table(load=True)\n",
    "user_table_encoded.head()\n",
    "# sku_map = pd.read_pickle('sku_map.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.371330Z",
     "start_time": "2021-04-20T22:58:35.311Z"
    }
   },
   "outputs": [],
   "source": [
    "# aggregate as dict INFO_Vector \n",
    "# Target structure user_ID:{attrs_combined: xxx__xxx__xxxx , request_list:'xxx__xxxx__xxxx__xxxx__xxxx', orginal_user_id}\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def load_train_dt(load=True, name='dt_train_v2.pkl'):\n",
    "    if load == False:\n",
    "        ### load dataset\n",
    "        user_table_encoded, user_map_dict = load_encode_user_table(load=load,nrows=None)\n",
    "        click_table = pd.read_csv('click_table.csv',usecols= [1,2], nrows=100000000) ## 这个table是我预处理过的table. 已经label encoding 过了\n",
    "        \n",
    "        ###  merge dataset\n",
    "        dt = pd.merge(user_table_encoded, click_table, how='left', on='user_ID')\n",
    "        # dt[~dt['sku_ID'].isnull()]\n",
    "        dt.dropna(how='any', inplace=True) # 是否需要?\n",
    "        dt[['sku_ID']]=dt[['sku_ID']].astype(int) # for convert to string, float type will contain dot zero\n",
    "        \n",
    "        \n",
    "        ### scan the table once and generate INFO_Vector for each user \n",
    "        INFO_Vector = defaultdict(lambda: {'attrs_combined':'','request_list':'', 'orginal_user_id':''})\n",
    "        for i in tqdm(range(len(dt))): \n",
    "            tmprec = {x[0]:x[1] for x in zip(dt.columns,dt.iloc[i])} # 当前行\n",
    "        #     INFO_Vector[tmprec['user_ID']]['attrs_combined']+=str(tmprec['user_encode'])+',' # 当前行信息储存到对应的 user INFO_Vector\n",
    "            INFO_Vector[tmprec['user_ID']]['request_list']+=str(tmprec['sku_ID'])+'__' # 当前行信息储存到对应的 user INFO_Vector\n",
    "        #     INFO_Vector[tmprec['user_ID']]['attrs_combined']+=str(tmprec['user_encode'])+',' # 当前行信息储存到对应的 user INFO_Vector\n",
    "\n",
    "        #     if tmprec['request_time_sec']==tmprec['request_time_sec']: # 判断不是 nan, 则\n",
    "        #         INFO_Vector[tmprec['user_ID']]['ts']+=str(tmprec['request_time_sec'])+','\n",
    "        #     else: # 是nan, 则找order_time\n",
    "        #         INFO_Vector[tmprec['user_ID']]['ts']+=str(tmprec['order_time_sec'])+','\n",
    "        #     INFO_Vector[tmprec['user_ID']]['neg']+=str()+','\n",
    "        # #     INFO_Vector[tmprec['user_ID']]['buy']+=str(tmprec['request_time_sec'])+','\n",
    "        #     INFO_Vector[tmprec['user_ID']]['order']+=str(tmprec['if_order'])+','\n",
    "        \n",
    "        #### update map for INFO_Vector\n",
    "        user_map_dict = user_table_encoded.set_index('user_ID').T.to_dict() # 这个转换过程特别慢. 但是后续合并很快\n",
    "        for i in tqdm(INFO_Vector.keys()):\n",
    "            INFO_Vector[i]['attrs_combined'] = user_map_dict[i]['user_encode']\n",
    "            INFO_Vector[i]['orginal_user_id'] = user_map_dict[i]['origin_user_ID']\n",
    "\n",
    "        dt_train = pd.DataFrame(INFO_Vector).T\n",
    "        dt_train.to_pickle(name) # 保存数据\n",
    "    else:\n",
    "        try:\n",
    "            dt_train = pd.read_pickle(name)\n",
    "        except:\n",
    "            print(\"Didn't save the file with this name before\")\n",
    "    return dt_train\n",
    "\n",
    "## 这是W2V的model数据\n",
    "dt_train = load_train_dt(load=True, name='dt_train.pkl')\n",
    "# dt_train.T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.371330Z",
     "start_time": "2021-04-20T22:58:35.314Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.484355Z",
     "start_time": "2021-04-20T22:58:35.480354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\OneDrive - stevens.edu\\\\Stevens DS\\\\CS609\\\\project\\\\RS_code\\\\preprocessing'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the orginal preprocessing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.499358Z",
     "start_time": "2021-04-20T22:58:35.485355Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rnn_dt_train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8e2ca9aaa324>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# 300MB 的pickle文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rnn_dt_train.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"infer\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# 1) try standard library Pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rnn_dt_train.pkl'"
     ]
    }
   ],
   "source": [
    "# a = load_click_order(num_samples=100000000, load=True, file_version='v1')\n",
    "# from collections import defaultdict\n",
    "# import numpy as np\n",
    "# # INFO_Vector = defaultdict(lambda: {'seq':'', 'ts':'','date':'', 'neg':'', 'buy':'', 'order':'', 'time_gap':'','last_time_request': 0.0})\n",
    "\n",
    "# INFO_Vector = defaultdict(lambda: {'seq':'', 'order':'', 'time_gap':'','last_time_request': 0.0})\n",
    "# for i in tqdm(range(len(a))): # 遍历数据表格一次, 保存所有信息\n",
    "#     tmprec = {x[0]:x[1] for x in zip(a.columns,a.iloc[i])} # 当前行\n",
    "#     INFO_Vector[tmprec['user_ID']]['seq']+=str(tmprec['sku_ID'])+',' # 当前行信息储存到对应的 user INFO_Vector\n",
    "# #     INFO_Vector[tmprec['user_ID']]['date']+=str(tmprec['year'])+'-'+str(tmprec['month'])+'-'+str(tmprec['day'])+','\n",
    "\n",
    "# #     if tmprec['request_time_sec']==tmprec['request_time_sec']: # 判断不是 nan, 则\n",
    "# #         INFO_Vector[tmprec['user_ID']]['ts']+=str(tmprec['request_time_sec'])+','\n",
    "# #     else: # 是nan, 则找order_time\n",
    "# #         INFO_Vector[tmprec['user_ID']]['ts']+=str(tmprec['order_time_sec'])+','\n",
    "# #     INFO_Vector[tmprec['user_ID']]['neg']+=str()+','\n",
    "# # #     INFO_Vector[tmprec['user_ID']]['buy']+=str(tmprec['request_time_sec'])+','\n",
    "#     INFO_Vector[tmprec['user_ID']]['order']+=str(tmprec['if_order'])+','\n",
    "    \n",
    "    \n",
    "#     # 上次时间 减去这次时间 # 会有负值, 因为是join一天, 有可能下单在request之前的错误\n",
    "#     INFO_Vector[tmprec['user_ID']]['time_gap'] += str(INFO_Vector[tmprec['user_ID']]['last_time_request'] -  tmprec['request_time_sec'])+','\n",
    "    \n",
    "#     # 更新时间记录\n",
    "#     INFO_Vector[tmprec['user_ID']]['last_time_request'] = tmprec['request_time_sec']\n",
    "\n",
    "\n",
    "# rnn_dt_train = pd.DataFrame(INFO_Vector).T\n",
    "# rnn_dt_train.to_pickle('rnn_dt_train.pkl') # 保存数据\n",
    "# rnn_dt_train\n",
    "\n",
    "\n",
    "# 300MB 的pickle文件\n",
    "a =  pd.read_pickle('rnn_dt_train.pkl')\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {user_ID: {\"seq\": sku_ID sequence}\n",
    "- with the help of nltk ngram function, generate training sample based on the diction structure data\n",
    "- window_size = 11 means: consider 11 request times. If one of them is ordered, one sample will be generated [context_sku_ID, center_sku_ID]\n",
    "- Detail: In the middle of 11 request time, the middle sku_ID (the six) is order, ten sku_ID around this center will be X, the center will be y \n",
    "    - If there are two order continuously, it means this sequence will generate two samples in window size = 11  \n",
    "    - <font color=red>理解为, 只要他买了一个物品, 周围10个request都是可能的商品, 生成样本的时候, 把其他都看成0</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Models - base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.623386Z",
     "start_time": "2021-04-20T22:58:35.602381Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'myutils_V4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2e9cf7880b2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmyutils_V4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'myutils_V4'"
     ]
    }
   ],
   "source": [
    "from myutils_V4 import *\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Input, Flatten, Concatenate\n",
    "from collections import Counter, defaultdict\n",
    "from gensim.models import word2vec\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import bigrams, trigrams , ngrams\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from sklearn import preprocessing\n",
    "# from tensorflow.keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from IPython.display import SVG\n",
    "from numpy.random import seed\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import gensim.downloader as api\n",
    "import glob\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import string, os \n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# tf.compat.v1.enable_eager_execution(config=None, device_policy=None, execution_mode=None)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # 关掉warning信息\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.624386Z",
     "start_time": "2021-04-20T22:58:35.573Z"
    }
   },
   "outputs": [],
   "source": [
    "# 300MB 的pickle文件\n",
    "rnn_dt_train =  pd.read_pickle('rnn_dt_train.pkl')\n",
    "# num_samples= 10000000\n",
    "\n",
    "\n",
    "num_samples = 100000000\n",
    "INFO_Vector = rnn_dt_train[0:num_samples]\n",
    "corpus = [i.split(',') for i in INFO_Vector['seq']] \n",
    "INFO_Vector = INFO_Vector.T.to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.625387Z",
     "start_time": "2021-04-20T22:58:35.576Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Clustering for user?\n",
    "# processer = TfidfVectorizer(max_df=1.0, min_df=5)    # 至少在5个文档中出现过\n",
    "# tfidf = processer.fit_transform(INFO_Vector['seq'])\n",
    "# aa = tfidf.toarray()\n",
    "# EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.627387Z",
     "start_time": "2021-04-20T22:58:35.579Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS_ratio = 0.95\n",
    "MAX_DOC_LEN_ratio = 0.90\n",
    "char_level_switch = False\n",
    "MAX_NB_WORDS = eda_MAX_NB_WORDS(corpus, ratio = MAX_NB_WORDS_ratio, filters=' ',char_level = char_level_switch)\n",
    "MAX_DOC_LEN = eda_MAX_DOC_LEN(corpus, ratio = MAX_DOC_LEN_ratio, filters=' ',char_level = char_level_switch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lots of information are drop\n",
    "- because most of user doesn't have enough request information \n",
    "- with more data, this process will be largely improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.300539Z",
     "start_time": "2021-04-20T22:58:35.713407Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_DOC_LEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3ac20a311c13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_DOC_LEN\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 80%的样本 request list 的长度, 除以2. 生成对应 n-gram样本. 并以中间为1的为一个样本\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuywhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 比这个短的直接没了\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msku_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MAX_DOC_LEN' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "window_size = int(MAX_DOC_LEN/2) # 80%的样本 request list 的长度, 除以2. 生成对应 n-gram样本. 并以中间为1的为一个样本\n",
    "\n",
    "def get_samples(tokens, buywhat, window_size): # 比这个短的直接没了\n",
    "    sku_list = ngrams(tokens, window_size)\n",
    "    order_list = ngrams(buywhat, window_size)\n",
    "    n_grams_sku =[]\n",
    "    for order_grams, sku_grams in zip(order_list, sku_list):\n",
    "        if order_grams[int(window_size/2)]=='1': # 如果中间这个词为1, 那么周围10个单位, 预测中间这个词 会买\n",
    "            X = list(sku_grams)\n",
    "            y = X.pop(int(window_size/2))\n",
    "            n_grams_sku.append([X, y]) # append([(x), y])\n",
    "    return n_grams_sku\n",
    "\n",
    "\n",
    "# get_samples(tokens, buywhat, window_size)\n",
    "\n",
    "ngram_samples = []\n",
    "ignore_set=0\n",
    "for i in list(INFO_Vector):\n",
    "    tokens = INFO_Vector[i]['seq'].split(',')[0:-1] \n",
    "    buywhat =  INFO_Vector[i]['order'].split(',')[0:-1] # 以逗号分隔, 然后去掉最后一个逗号\n",
    "#     print(tokens,buywhat)\n",
    "    tmp_sample = get_samples(tokens, buywhat, window_size)\n",
    "    if len(tmp_sample)>=1: # 如果不为空, 则填入到 training data\n",
    "        ngram_samples.extend(tmp_sample)\n",
    "    else:\n",
    "        ignore_set+=1 # 计数, 丢掉了多少个user信息\n",
    "        \n",
    "print(\"{} user information drop: About ({:.2f}%) \".format(ignore_set, ignore_set/(len(INFO_Vector))*100))\n",
    "\n",
    "docs = pd.DataFrame(ngram_samples,columns=['X','y'])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and genarate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.302539Z",
     "start_time": "2021-04-20T22:58:35.709Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# 用sparse categorical loss 就不用对y进行one hot\n",
    "\n",
    "test_ratio = 0.1\n",
    "seed=2\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(docs['X'],docs['y'],test_size=test_ratio, random_state=seed)\n",
    "processor = text_preprocessor(MAX_DOC_LEN, MAX_NB_WORDS, docs['X'])\n",
    "\n",
    "x_train = processor.generate_seq(x_train)\n",
    "# y_train = to_categorical(y_train)\n",
    "y_train = y_train.astype(int)\n",
    "x_test = processor.generate_seq(x_test)\n",
    "# y_test = to_categorical(y_test)\n",
    "y_test = y_test.astype(int)\n",
    "print('Shape of x_tr: ' + str(x_train.shape))\n",
    "print('Shape of y_tr: ' + str(y_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))\n",
    "\n",
    "output_shape = max(y_train)+1 # 为了满足sparse categorical loss的计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.303540Z",
     "start_time": "2021-04-20T22:58:35.711Z"
    }
   },
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def Best_model_report(grid_result, to_file='GV_result.xlsx' ):\n",
    "    GV_result = pd.DataFrame(grid_result.cv_results_)\n",
    "    GV_result.to_excel(to_file)\n",
    "#     y_pred = grid_result.predict(x_test)\n",
    "#     y_test_one=np.argmax(y_test,axis=1)\n",
    "#     cm = confusion_matrix(y_test_one, y_pred)\n",
    "#     print('confusion matrix:\\n', cm)\n",
    "#     print('classification report:\\n', classification_report(y_test_one, y_pred))\n",
    "    return GV_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.304540Z",
     "start_time": "2021-04-20T22:58:35.714Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, BATCH_SIZE, NUM_EPOCHES, BestModel_Name=\"best_model\", patience=10 ): # Final one step\n",
    "    \n",
    "    #### Best model selection \n",
    "    BEST_MODEL_FILEPATH = BestModel_Name\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='min') # patience: number of epochs with no improvement on monitor : val_loss\n",
    "    # monitoring\n",
    "    checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    history = model.fit(x_train, y_train, validation_split=0.2, batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, callbacks=[earlyStopping, checkpoint], verbose=2)\n",
    "    model.load_weights(BestModel_Name)\n",
    "\n",
    "    #### classification Report\n",
    "    history_plot(history)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred>0.5))\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print( \"\\n\\n\\n\")\n",
    "    return y_pred # 也许能出 tpr 和 fpr图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.305541Z",
     "start_time": "2021-04-20T22:58:35.717Z"
    }
   },
   "outputs": [],
   "source": [
    "# define Model for classification\n",
    "def model_Create(FS, NF, EMB, MDL, MNW, PWV = None, optimizer='RMSprop', trainable_switch=True):\n",
    "    cnn_box = cnn_model(FILTER_SIZES=FS, MAX_NB_WORDS=MNW, MAX_DOC_LEN=MDL, EMBEDDING_DIM=EMB, NUM_FILTERS=NF, PRETRAINED_WORD_VECTOR=PWV, trainable_switch=trainable_switch)\n",
    "    q1_input = Input(shape=(MDL,), dtype='int32', name='q1_input') # Hyperparameters: MAX_DOC_LEN\n",
    "    encode_input1 = cnn_box(q1_input)\n",
    "    half_features = int(len(FS)*NF/2)\n",
    "    dense1 = Dense(half_features,activation='relu', name='half_features')(encode_input1)\n",
    "    drop_1 = Dropout(rate=0.4, name='dropout')(dense1)\n",
    "    pred = Dense(output_shape,activation='softmax', name='Prediction')(drop_1)\n",
    "    \n",
    "    model = Model(inputs=q1_input, outputs=pred)    \n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# model = model_Create(FS=[2,3,4], NF=12, EMB=200, MDL=19, MNW=2126, PWV = CBOW_W2V,trainable_switch=False)\n",
    "# model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T17:19:01.170629Z",
     "start_time": "2020-05-12T17:19:01.166629Z"
    }
   },
   "source": [
    "## Explaination \n",
    "- When predicting, based on user request id sequence, recommend the order sku_id generated by models\n",
    "- <font color=red> Only need to review the score of the evaluation result for each RNNs</font> The classification report is the CNNs result. I forget to skip it.\n",
    "- CNNs: 92.80%\n",
    "- RNNs: 0.807, 0.77538645, 0.7691645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained CBOW_W2V for sku_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.306541Z",
     "start_time": "2021-04-20T22:58:35.854Z"
    }
   },
   "outputs": [],
   "source": [
    "EMB = [100]\n",
    "iter_step= 300\n",
    "CBOW_W2V =  processor.w2v_pretrain(EMB[0], min_count=2, seed=1, cbow_mean=1,negative=5, window=5, iter=iter_step, workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs\n",
    "- the parameters can be editted into grid search version\n",
    "- But there is a bug need to be handle later\n",
    "    - Pretrained embedding cannot be fixed in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.307541Z",
     "start_time": "2021-04-20T22:58:35.924Z"
    }
   },
   "outputs": [],
   "source": [
    "patience = 10\n",
    "epoch = 30\n",
    "n_jobs = 1 # if use GPU, this have to be one.\n",
    "\n",
    "file_name = 'test'\n",
    "BestModel_Name = file_name+ 'Best_GS'\n",
    "\n",
    "############# Set hyper parameters\n",
    "FILTER_SIZES= [4,5,6,7,8]\n",
    "NUM_FILTERS=24\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE=128 # increase speed with large batch size and avoid overfit or wrong direction\n",
    "NUM_EPOCHES=20 # patience=20\n",
    "# CBOW_W2V = processor.w2v_pretrain(EMBEDDING_DIM) # 需要train, 比较慢\n",
    "# Glove_W2V = processor.load_glove_w2v(EMBEDDING_DIM) # 需要下载, 比较慢\n",
    "OPT = optimizers.Adam(lr=1e-4)\n",
    "trainable_switch=False\n",
    "\n",
    "\n",
    "model = model_Create(FS=FILTER_SIZES, NF=NUM_FILTERS, MDL=MAX_DOC_LEN,MNW=MAX_NB_WORDS+1, EMB=EMBEDDING_DIM, PWV = CBOW_W2V, trainable_switch=trainable_switch, optimizer=OPT )\n",
    "# model_best_1_pred = train_model(model, x_train, y_train, x_test, y_test, BATCH_SIZE, NUM_EPOCHES, BestModel_Name=BestModel_Name)\n",
    "# model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.307541Z",
     "start_time": "2021-04-20T22:58:35.927Z"
    }
   },
   "outputs": [],
   "source": [
    "BEST_MODEL_FILEPATH = BestModel_Name\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='min') # patience: number of epochs with no improvement on monitor : val_loss\n",
    "# monitoring\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "history = model.fit(x_train, y_train, validation_split=0.2, batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, callbacks=[earlyStopping, checkpoint], verbose=2)\n",
    "model.load_weights(BestModel_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.308541Z",
     "start_time": "2021-04-20T22:58:35.930Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### classification Report\n",
    "history_plot(history)\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T21:23:58.614653Z",
     "start_time": "2020-05-11T21:23:38.196733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T21:23:58.948740Z",
     "start_time": "2020-05-11T21:23:58.615652Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.310541Z",
     "start_time": "2021-04-20T22:58:36.016Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# 用sparse categorical loss 就不用对y进行one hot\n",
    "\n",
    "test_ratio = 0.1\n",
    "seed=2\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(docs['X'],docs['y'],test_size=test_ratio, random_state=seed)\n",
    "processor = text_preprocessor(MAX_DOC_LEN, MAX_NB_WORDS, docs['X'])\n",
    "\n",
    "x_train = processor.generate_seq(x_train)\n",
    "# y_train = to_categorical(y_train)\n",
    "y_train = y_train.astype(int)\n",
    "x_test = processor.generate_seq(x_test)\n",
    "# y_test = to_categorical(y_test)\n",
    "y_test = y_test.astype(int)\n",
    "print('Shape of x_tr: ' + str(x_train.shape))\n",
    "print('Shape of y_tr: ' + str(y_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))\n",
    "\n",
    "output_shape = max(y_train)+1 # 为了满足sparse categorical loss的计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.311541Z",
     "start_time": "2021-04-20T22:58:36.019Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"MAX_DOC_LEN\", MAX_DOC_LEN)\n",
    "print(\"MAX_NB_WORDS\", MAX_NB_WORDS)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.311541Z",
     "start_time": "2021-04-20T22:58:36.022Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.312541Z",
     "start_time": "2021-04-20T22:58:36.025Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "latent_dim = 32\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "### construct the RNN with GRU unit\n",
    "model_0 = Sequential()\n",
    "model_0.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM)) #  embedding dimension , 这里的输入应该是 one-hot 的19个词. 还是直接sequence. 都行\n",
    "model_0.add(LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.5,return_sequences=True))\n",
    "model_0.add(LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.5,return_sequences=True))\n",
    "model_0.add(LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.5,return_sequences=False))\n",
    "model_0.add(Dropout(0.4))\n",
    "model_0.add(Dense(output_shape, activation='softmax')) # 因为 y 是经过 one-hot 的, 所以他能保存位置信息.\n",
    "\n",
    "model_0.summary()\n",
    "model_0.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "history = model_0.fit(x_train, y_train, epochs=10,  batch_size=128, validation_split=0.1,  shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.313542Z",
     "start_time": "2021-04-20T22:58:36.028Z"
    }
   },
   "outputs": [],
   "source": [
    "#### classification Report\n",
    "history_plot(history)\n",
    "# y_pred = model.predict(x_test) # 内存不够\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model_0.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:58:13.526929Z",
     "start_time": "2020-05-07T00:58:13.523927Z"
    }
   },
   "source": [
    "### Inverse encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.314541Z",
     "start_time": "2021-04-20T22:58:36.113Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RNN_recommend_result =[] \n",
    "\n",
    "for seq in tqdm(x_test):\n",
    "    sku_record = []\n",
    "    for i in seq:\n",
    "        sku_record.append(processor.index_word[i]) # 返回商品顺序\n",
    "    idx = np.argmax(model.predict(seq.reshape(1,-1)),axis=1)[0] # 找到最大概率的商品\n",
    "    sku_record.append(idx)\n",
    "    RNN_recommend_result.append(sku_record)\n",
    "    \n",
    "RNN_recommend_result = pd.DataFrame(RNN_recommend_result)\n",
    "RNN_recommend_result.columns = list(RNN_recommend_result.columns[0:-1])+['recommend']\n",
    "RNN_recommend_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.316545Z",
     "start_time": "2021-04-20T22:58:36.199Z"
    }
   },
   "outputs": [],
   "source": [
    "### construct the RNN with GRU unit\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM)) #  embedding dimension , 这里的输入应该是 one-hot 的19个词. 还是直接sequence. 都行\n",
    "model_1.add(Bidirectional(LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.2,return_sequences=False)))\n",
    "model_1.add(Dropout(0.4))\n",
    "model_1.add(Dense(output_shape, activation='softmax')) # 因为 y 是经过 one-hot 的, 所以他能保存位置信息.\n",
    "\n",
    "model_1.summary()\n",
    "model_1.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "history = model_1.fit(x_train, y_train, epochs=10,  batch_size=128, validation_split=0.1,  shuffle=True) \n",
    "\n",
    "\n",
    "#### classification Report\n",
    "history_plot(history)\n",
    "# y_pred = model_1.predict(x_test) # 内存不够\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model_1.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model_1.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.317543Z",
     "start_time": "2021-04-20T22:58:36.202Z"
    }
   },
   "outputs": [],
   "source": [
    "### construct the RNN with GRU unit\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM)) #  embedding dimension , 这里的输入应该是 one-hot 的19个词. 还是直接sequence. 都行\n",
    "model_2.add((LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.2,return_sequences=False)))\n",
    "model_2.add(Dropout(0.4))\n",
    "model_2.add(Dense(output_shape, activation='softmax')) # 因为 y 是经过 one-hot 的, 所以他能保存位置信息.\n",
    "\n",
    "model_2.summary()\n",
    "model_2.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "history = model_2.fit(x_train, y_train, epochs=10,  batch_size=128, validation_split=0.1,  shuffle=True) \n",
    "\n",
    "#### classification Report\n",
    "history_plot(history)\n",
    "# y_pred = model_2.predict(x_test) # 内存不够\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model_2.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model_2.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.318543Z",
     "start_time": "2021-04-20T22:58:36.205Z"
    }
   },
   "outputs": [],
   "source": [
    "### construct the RNN with GRU unit\n",
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM)) #  embedding dimension , 这里的输入应该是 one-hot 的19个词. 还是直接sequence. 都行\n",
    "model_3.add((GRU(latent_dim, dropout=0.0, recurrent_dropout=0.2,return_sequences=False)))\n",
    "model_3.add(Dropout(0.4))\n",
    "model_3.add(Dense(output_shape, activation='softmax')) # 因为 y 是经过 one-hot 的, 所以他能保存位置信息.\n",
    "\n",
    "model_3.summary()\n",
    "model_3.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "history = model_3.fit(x_train, y_train, epochs=10,  batch_size=128, validation_split=0.1,  shuffle=True) \n",
    "\n",
    "\n",
    "\n",
    "#### classification Report\n",
    "history_plot(history)\n",
    "# y_pred = model_3.predict(x_test) # 内存不够\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model_3.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model_3.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T05:32:45.939723Z",
     "start_time": "2020-05-06T05:32:45.935139Z"
    }
   },
   "source": [
    "# Other EDA - Tableau could handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:23:45.268077Z",
     "start_time": "2020-05-06T17:23:45.264077Z"
    }
   },
   "source": [
    "## Pie - platform distributionm\n",
    "- Color map: https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.417566Z",
     "start_time": "2021-04-20T22:58:36.414565Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_CLICK = './data/JD_click_data.csv'\n",
    "PATH_USER = './data/JD_user_data.csv'\n",
    "PATH_SKU = './data/JD_sku_data.csv'\n",
    "PATH_ORDER = './data/JD_order_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.454573Z",
     "start_time": "2021-04-20T22:58:36.419566Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File order_table.csv does not exist: 'order_table.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9bba901ef828>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0morder_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'order_table.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# order_table.to_csv('order_table_tmp.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# order_table.to_csv('order_table.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File order_table.csv does not exist: 'order_table.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "order_table = pd.read_csv('order_table.csv',nrows=100000)\n",
    "# order_table.to_csv('order_table_tmp.csv')\n",
    "# order_table.to_csv('order_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.456574Z",
     "start_time": "2021-04-20T22:58:36.371Z"
    }
   },
   "outputs": [],
   "source": [
    "click_table = pd.read_csv('click_table.csv',nrows=100000)\n",
    "# click_table.to_csv('click_table_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.457574Z",
     "start_time": "2021-04-20T22:58:36.374Z"
    }
   },
   "outputs": [],
   "source": [
    "click_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.458575Z",
     "start_time": "2021-04-20T22:58:36.377Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_3 = click_table.groupby(['channel']).count()\n",
    "df_3.iloc[:,0].plot(\n",
    "    kind='pie',\n",
    "    table=df_3.iloc[:,1],\n",
    "    autopct='%1.1f%%', cmap='Set3',figsize=(8,8)\n",
    ")\n",
    "\n",
    "# View the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "347.587px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
