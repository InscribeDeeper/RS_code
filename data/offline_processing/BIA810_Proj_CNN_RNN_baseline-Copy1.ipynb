{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:33.924496Z",
     "start_time": "2021-04-20T22:58:33.922496Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T05:59:32.396469Z",
     "start_time": "2021-04-19T05:59:32.311Z"
    }
   },
   "source": [
    "这里要写db的代码去读 数据库中 当天的数据\n",
    "- 如果那个map的key不存在, 则数据库+1\n",
    "- 需要设定当前日期, 然后数据库就会不断 accumulate 到当日\n",
    "- 目前这样的model 适合 长线的数据分析, 不适合RS的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T23:00:42.584622Z",
     "start_time": "2021-04-20T23:00:42.579620Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\py810\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import datatable\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "dt_folder = \"../data/\"\n",
    "output_folder = \"../processed_data/\"\n",
    "PATH_CLICK = dt_folder+'JD_click_data.csv'\n",
    "PATH_USER = dt_folder+'JD_user_data.csv'\n",
    "PATH_SKU = dt_folder+'JD_sku_data.csv'\n",
    "PATH_ORDER = dt_folder+'JD_order_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T05:34:44.925230Z",
     "start_time": "2020-05-11T05:34:44.923238Z"
    }
   },
   "source": [
    "# DB function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.031295Z",
     "start_time": "2021-04-20T22:58:35.020250Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function to load data : not DB version\n",
    "# =============================================================================\n",
    "\n",
    "def load_daily_data(now= \"2018-03-15\", date_field='order_date', file_path=\"\", Filter = True, from_start = False):       \n",
    "    '''\n",
    "    load click table\n",
    "    input: previous date, now\n",
    "    >>> load_click(sort=['user_ID', 'request_time'])\n",
    "    \n",
    "    it should be the data manipulation on Database\n",
    "    we can write the server side python code to extract similar dataset \n",
    "    but now, I am not that familiar with the pymongo yet. Used pandas to replace the function\n",
    "    增删改查都需要 写对应的function\n",
    "    \n",
    "    具体的数据 filter 可以在这里添加\n",
    "    '''\n",
    "    # to be replaced by DB query\n",
    "    df = datatable.fread(file_path).to_pandas()\n",
    "    \n",
    "    # error checking\n",
    "    if date_field not in df.columns:\n",
    "        print(df.columns)\n",
    "        raise AttributeError(\"data field not in columns. plz check\")\n",
    "        \n",
    "    # time selection\n",
    "    if from_start is False:\n",
    "        predate = (pd.to_datetime(now) + datetime.timedelta(days = -1)).strftime('%Y-%m-%d') # both package works for time manipulation\n",
    "    else:\n",
    "        predate = (pd.to_datetime(now) +  relativedelta(months=-360)).strftime('%Y-%m-%d') # both package works for time manipulation\n",
    "    # df['request_date'] = df['request_time'].apply(lambda x: datetime(x.year, x.month, x.day)) # saved in DB field\n",
    "    df = df[(df[date_field]>predate) & (df[date_field]<=now)]        \n",
    "    \n",
    "    # filter for memory limitation\n",
    "    if Filter:\n",
    "#         user_tab = load_user()\n",
    "#         sku_tab = load_sku()\n",
    "#         user_tab.sample(n=50000, random_state=1).to_csv(output_folder+'target_user.csv')\n",
    "#         sku_tab.sample(n=3000, random_state=1).to_csv(output_folder+'target_sku.csv')\n",
    "        df = df[df['user_ID']!='-'] # delete \"-\" user \n",
    "        target_sku = pd.read_csv(output_folder+'target_sku.csv', index_col=\"sku_ID\")[['type', 'brand_ID', 'attribute1', 'attribute2']]\n",
    "        target_user = pd.read_csv(output_folder+'target_user.csv', index_col=\"user_ID\")[['user_level', 'first_order_month', 'plus', 'gender',\n",
    "                                                                           'age', 'marital_status', 'education', 'city_level', 'purchase_power']]\n",
    "        print(\"target sku rows \", target_sku.shape[0])\n",
    "        print(\"target user rows \", target_user.shape[0])\n",
    "        df = df.merge(target_sku, how='inner', on='sku_ID')\n",
    "        df = df.merge(target_user, how='inner', on='user_ID')\n",
    "\n",
    "\n",
    "    return df \n",
    "\n",
    "def load_user(PATH_USER=PATH_USER):\n",
    "    return pd.read_csv(PATH_USER, index_col=\"user_ID\")\n",
    "\n",
    "def load_sku(PATH_SKU=PATH_SKU):\n",
    "    return pd.read_csv(PATH_SKU, index_col=\"sku_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.044255Z",
     "start_time": "2021-04-20T22:58:35.033253Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# utils\n",
    "# =============================================================================\n",
    "\n",
    "def ts_str2sec(format_time):\n",
    "    '''\n",
    "    input: format_time = \"2018-03-01 13:21:04\"\n",
    "    output: timeStamp = 1381419600\n",
    "    '''\n",
    "    ts = time.strptime(format_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return time.mktime(ts)  \n",
    "\n",
    "def ts_sec2str(timeStamp):\n",
    "    '''\n",
    "    input:  timeStamp = 1381419600\n",
    "    output: format_time = \"2018-03-01 13:21:04\"\n",
    "    '''\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(timeStamp))\n",
    "\n",
    "def ts_attrs_add(df, ts_col = 'request_time'):\n",
    "    # num_samples=10000\n",
    "    # ######### generate request_date\n",
    "    # df = read_csv(PATH_CLICK, nrows=num_samples)\n",
    "    # ts_col = 'request_time'\n",
    "    df[ts_col] = to_datetime(df[ts_col])\n",
    "    df[ts_col+'_sec'] = df[ts_col].astype(str).progress_apply(ts_str2sec)\n",
    "    \n",
    "    # For visulization\n",
    "    df['hour'] = df[ts_col].dt.hour\n",
    "    df['day'] = df[ts_col].dt.day\n",
    "    df['month'] = df[ts_col].dt.month\n",
    "    df['year'] = df[ts_col].dt.year\n",
    "    df['daysinmonth'] = df[ts_col].dt.daysinmonth\n",
    "    df['dayofyear'] = df[ts_col].dt.dayofyear \n",
    "    \n",
    "    # year-month-day\n",
    "    df[ts_col[0:-4]+'date'] = df[ts_col].dt.date\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate user-item matrix by day\n",
    "- it can be done by query as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.055258Z",
     "start_time": "2021-04-20T22:58:35.046255Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_user_item_matrix(daily_dt, based='click'):\n",
    "    dt = daily_dt\n",
    "    \n",
    "    if based == \"click\":\n",
    "        dt = dt.groupby(by=['user_ID','sku_ID']).count().reset_index() # grouping by day\n",
    "        dt = dt.pivot_table(index='user_ID', columns='sku_ID', values='request_time') # panel data \n",
    "    elif based == \"order\":\n",
    "        dt = dt.groupby(by=['user_ID','sku_ID'])['quantity'].sum().reset_index() # grouping by day\n",
    "        dt = dt.pivot_table(index='user_ID', columns='sku_ID', values='quantity') # panel data \n",
    "        \n",
    "    dt = dt.fillna(0).astype(\"int16\") # NaN value imputing # large upcast\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:24:27.328896Z",
     "start_time": "2021-04-19T18:24:27.326895Z"
    }
   },
   "source": [
    "## update user_item_matrix_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.066260Z",
     "start_time": "2021-04-20T22:58:35.056258Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_user_item_matrix_by_day(user_item_from_start, user_item_daily):\n",
    "    '''\n",
    "    user_item_from_start = big matrix on DB\n",
    "    user_item_daily = daily computed matrix \n",
    "    # 这里需要考虑的是, 这里不是电影, 电影看过一遍不会看了, 商品仍然会click\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # user_item_from_start.sum()['1c1453e829']\n",
    "    # incremental_tab.sum()['1c1453e829']\n",
    "    # updated.sum()['1c1453e829']\n",
    "\n",
    "    incremental_tab = (user_item_from_start*0) # preserve the index and column but with zero value\n",
    "    incremental_tab.update(user_item_daily, overwrite=True) # update the target part\n",
    "    updated = (user_item_from_start + incremental_tab)\n",
    "    return updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measure user similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.078263Z",
     "start_time": "2021-04-20T22:58:35.067261Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_item_sim(updated_user_item_matrix, Filter=True):\n",
    "    '''we can use cosine similarity as well'''\n",
    "    if Filter:\n",
    "        user_sets = updated_user_item_matrix.sum(axis=1).nlargest(1000).index # only consider top 1000 active user to get the item similarity\n",
    "        updated_user_item_matrix = updated_user_item_matrix.loc[user_sets]\n",
    "    \n",
    "    mat = updated_user_item_matrix\n",
    "    assert mat.columns.names[0] == \"sku_ID\"\n",
    "    return mat.corr()\n",
    "    \n",
    "def get_user_sim(updated_user_item_matrix, Filter=True):\n",
    "    if Filter:\n",
    "        item_sets = updated_user_item_matrix.sum(axis=0).nlargest(100).index # only consider top 100 popular items to get the user similarity\n",
    "        updated_user_item_matrix = updated_user_item_matrix[item_sets]\n",
    "        \n",
    "    mat = updated_user_item_matrix.T\n",
    "    assert mat.columns.names[0] == \"user_ID\"\n",
    "    return mat.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate prediction matrix for one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.085264Z",
     "start_time": "2021-04-20T22:58:35.079263Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_topK_idx(x, topK):\n",
    "    if isinstance(x, pd.Series):\n",
    "        x = x.tolist()\n",
    "    #  or isinstance(x, np.ndarray)\n",
    "    idx = np.argsort(x)[::-1][0:topK]\n",
    "    return idx\n",
    "\n",
    "# def _get_topK_for_each(user, updated_user_item_matrix, user_cor, topNeighbor = 1000, rs_topItem = 10):\n",
    "#     '''\n",
    "#     # test code for one user\n",
    "#     rs_topItem = 10\n",
    "#     topNeighbor = 1000\n",
    "#     user = updated_user_item_matrix.index[0]\n",
    "#     updated_user_item_matrix = update_user_item_matrix_by_day(user_item_from_start, user_item_daily)\n",
    "#     user_cor = get_user_sim(updated_user_item_matrix)\n",
    "\n",
    "#     get_topK_for_each(user, updated_user_item_matrix=updated_user_item_matrix, user_cor=user_cor, topNeighbor = 1000, rs_topItem = 10)\n",
    "    \n",
    "#     '''\n",
    "#     sim_mat = user_cor.loc[user].nlargest(topNeighbor) # select one column and get the most similar neighbors\n",
    "#     topN_sim_user = sim_mat.index\n",
    "#     r_bp = updated_user_item_matrix.loc[topN_sim_user]\n",
    "#     r_delta = r_bp - r_bp.mean(axis=1).values.reshape(-1,1)\n",
    "#     # updated_user_item_matrix.loc[user].mean() + np.average(b_hat, weights=sim_mat, axis=0)\n",
    "#     pred_for_user = updated_user_item_matrix.loc[user].mean() + np.dot(sim_mat.values, r_delta.values) / sim_mat.values.sum()\n",
    "    \n",
    "#     ## get top K\n",
    "#     topK_idx = np.argsort(pred_for_user)[::-1][0:rs_topItem]\n",
    "#     topK_ID = updated_user_item_matrix.columns[topK_idx].tolist() # extract ID from input matrix\n",
    "#     return topK_ID\n",
    "\n",
    "# # rs_pred = list(map(lambda user: get_topK_for_each(user, updated_user_item_matrix=updated_user_item_matrix, user_cor=user_cor, topNeighbor = 1000, rs_topItem = 10), updated_user_item_matrix.index.values))\n",
    "# user_cor = get_user_sim(updated_user_item_matrix)\n",
    "# rs_pred = {}\n",
    "# for x_user in tqdm(updated_user_item_matrix.index.values):\n",
    "#     rs_pred[x_user] = get_topK_for_each(x_user, updated_user_item_matrix=updated_user_item_matrix, user_cor=user_cor, topNeighbor = 1000, rs_topItem = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.096267Z",
     "start_time": "2021-04-20T22:58:35.087265Z"
    }
   },
   "outputs": [],
   "source": [
    "# import swifter # only works on linux system\n",
    "\n",
    "def get_score(x, updated_user_item_matrix, users_sim):\n",
    "    r_bp = updated_user_item_matrix.loc[x.values[0]] # retrieve similar use index by x.values[0] : nTop_user x all_items\n",
    "    r_delta = r_bp - r_bp.mean(axis=1).values.reshape(-1,1) # calc average click and delta for b item : nTop_user x all_items\n",
    "    res = updated_user_item_matrix.loc[x.name].mean() + np.dot(users_sim.loc[x.name], r_delta.values) / np.sum(users_sim.loc[x.name])# : nTop_user x all_items dot nTop_user_similarity_score\n",
    "    return res\n",
    "\n",
    "def get_u_pred_map(updated_user_item_matrix, topNeighbor=100, rs_topItem=10):\n",
    "    '''user based method'''\n",
    "    '''build a map that: given an user, return recommendate items'''\n",
    "    user_cor = get_user_sim(updated_user_item_matrix, Filter=True)\n",
    "    topN_sim_users = user_cor.apply(lambda x: x.index[get_topK_idx(x, topNeighbor)].tolist(), axis=1).to_frame()\n",
    "    users_sim = user_cor.apply(lambda x: x.values[get_topK_idx(x, topNeighbor)], axis=1) \n",
    "    # users_sim = user_cor.apply(lambda x: pd.Series(x.values[get_topK_idx(x, topNeighbor)]), axis=1) \n",
    "    score = topN_sim_users.progress_apply(lambda x: get_score(x, updated_user_item_matrix=updated_user_item_matrix, users_sim=users_sim), axis=1) # x 是一个user_ID\n",
    "    pred_utab = score.apply(lambda x: updated_user_item_matrix.columns[get_topK_idx(x, rs_topItem)].tolist()) # get top items index by \"np.argsort(x)[::-1][0:rs_topItem]\"\n",
    "    return pred_utab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item based recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.244301Z",
     "start_time": "2021-04-20T22:58:34.440Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def get_i_pred_map(updated_user_item_matrix, topNeighbor=100, rs_topItem=10):\n",
    "    '''item based method'''\n",
    "    '''build a map that: given an item, return recommendate items'''\n",
    "    \n",
    "    updated_iu_mtx = updated_user_item_matrix.T\n",
    "    idx = updated_iu_mtx.index\n",
    "    model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_jobs=-1).fit(updated_iu_mtx)\n",
    "    # topNeighbor = 10\n",
    "    distances, indices = model_knn.kneighbors(updated_iu_mtx, n_neighbors=topNeighbor) # compute Knearest for each item\n",
    "    d = dict(zip(idx, map(lambda x: idx[x], indices))) # reverse index\n",
    "\n",
    "    # get item similarity, which can also be used to make recommendation when input a item\n",
    "    item_KNN_prediction = pd.DataFrame.from_dict(d, orient='index', columns=[\"top\" + str(x) for x in range(1,topNeighbor+1)]) \n",
    "    pred_itab = item_KNN_prediction\n",
    "\n",
    "    return pred_itab\n",
    "\n",
    "\n",
    "\n",
    "def get_u_pred_map2(updated_user_item_matrix, topNeighbor=100, rs_topItem=10):\n",
    "    '''item based method '''\n",
    "    '''build a map that: given an item, return recommendate items'''\n",
    "    updated_iu_mtx = updated_user_item_matrix.T\n",
    "    item_KNN_prediction = get_i_pred_map(updated_user_item_matrix, topNeighbor=100, rs_topItem=10)\n",
    "    # get neighbors items average clicks  \n",
    "        ## 1.找到最相似的10个item 保存到KNN_prediction tab中;  2. 找到那10个item在总表中每个users的得分, 计算其均值, 输出一个行向量 (每一个element是一个user的对这个item的 得分均值)\n",
    "    score = item_KNN_prediction.apply(lambda x: updated_iu_mtx.loc[x.tolist()].mean(), axis=1)\n",
    "    # get top K based on neighbors items average clicks  \n",
    "        ## 3.对每一个user, 找到 均值得分 排名最高的20个item\n",
    "    # rs_topItem = 20 \n",
    "        ## np.argsort(x.values) 必须用values, 因为如果x是pd.series, 则会根据 key 的字符串 去排序\n",
    "    pred_utab = score.T.apply(lambda x:x.index[get_topK_idx(x, rs_topItem)].tolist(), axis=1) # 取出来的是一个series, 所以需要用index, 而不是columns (虽然x是一行)\n",
    "    return pred_utab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.246302Z",
     "start_time": "2021-04-20T22:58:34.520Z"
    }
   },
   "outputs": [],
   "source": [
    "# offline 每天训练一个knn, 并且找到最近的item, 基于这些item的平均click, 去预测这个用户明天的推荐清单b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-20T23:01:41.009Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\");\n",
    "all_rec = load_daily_data(now= \"2018-04-1\", date_field='request_time', file_path=PATH_CLICK, Filter=False, from_start=True)\n",
    "group_rec = all_rec.groupby(\"sku_ID\").count()['user_ID']\n",
    "group_rec.sort_values().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main func -  CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start-up \n",
    "- only run once for system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:59:03.434283Z",
     "start_time": "2021-04-20T22:59:03.430282Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 0:  fake initialization to a time t0 \"2018-03-13\"\n",
    "test = False\n",
    "if test:\n",
    "    rec_from_start = load_daily_data(now= \"2018-03-13\", date_field='request_time', file_path=PATH_CLICK, Filter=True, from_start=True)\n",
    "    user_item_from_start = generate_user_item_matrix(rec_from_start, based='click')\n",
    "         ## should be replaced with DB QUERY ## factorization could be applied to save storage ## some rollback mechanism should implemented on DB level\n",
    "    user_item_from_start.to_csv(output_folder+'CF_click/today/user_item_from_start.csv') \n",
    "    pd.read_csv(output_folder+'CF_click/today/user_item_from_start.csv',index_col = \"user_ID\").to_csv(output_folder+'CF_click/predate/user_item_from_start.csv') # copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily offline process - server side\n",
    "- DB version should be used to replace this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T23:01:14.644989Z",
     "start_time": "2021-04-20T23:01:00.648869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target sku rows  3000\n",
      "target user rows  50000\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: load saved user_item_from_start from previous day.\n",
    "    ## should be replaced with DB QUERY\n",
    "user_item_from_start = pd.read_csv(output_folder+'CF_click/predate/user_item_from_start.csv', index_col='user_ID')\n",
    "\n",
    "# STEP 2: generate daily incremental data offline for today\n",
    "rec_daily = load_daily_data(now= \"2018-03-13\", date_field='request_time', file_path=PATH_CLICK)\n",
    "user_item_daily = generate_user_item_matrix(rec_daily, based='click')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T23:01:22.292494Z",
     "start_time": "2021-04-20T23:01:16.791102Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 3: update user_item_from_start and saved into DB\n",
    "updated_user_item_matrix = update_user_item_matrix_by_day(user_item_from_start, user_item_daily)\n",
    "updated_user_item_matrix.to_csv(output_folder+'CF_click/today/user_item_from_start.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-20T23:01:22.966Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar:  87%|█████████████████████████████████████████████████████████▌        | 7059/8094 [18:51<02:52,  6.01it/s]"
     ]
    }
   ],
   "source": [
    "# STEP 4: compute recommendation item matrix for each user\n",
    "rs_topItem = 10\n",
    "topNeighbor = 100\n",
    "u_rs_pred = get_u_pred_map(updated_user_item_matrix, topNeighbor, rs_topItem)\n",
    "i_rs_pred = get_i_pred_map(updated_user_item_matrix, topNeighbor, rs_topItem)\n",
    "u_rs_pred2 = get_u_pred_map2(updated_user_item_matrix, topNeighbor, rs_topItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-20T23:01:51.766Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5: saved rs_pred_json into DB & DB copy finish step\n",
    "    # it should be used on online env \n",
    "rs_pred_json = json.dumps(rs_pred)\n",
    "f = open(output_folder+'CF_click/online_rs_dict.json', 'w')\n",
    "f.write(rs_pred_json)\n",
    "f.close()\n",
    "\n",
    "pd.read_csv(output_folder+'CF_click/today/user_item_from_start.csv',index_col = \"user_ID\").to_csv(output_folder+'CF_click/predate/user_item_from_start.csv') # copy\n",
    "print(\"Done for one day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main func - for order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start-up \n",
    "- only run once for system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.253303Z",
     "start_time": "2021-04-20T22:58:34.996Z"
    }
   },
   "outputs": [],
   "source": [
    "# STEP 0:  fake initialization to a time t0 \"2018-03-13\"\n",
    "rec_from_start = load_daily_data(now= \"2018-03-13\", date_field='request_time', file_path=PATH_CLICK, from_start=True)\n",
    "user_item_from_start = generate_user_item_matrix(rec_from_start, based='click')\n",
    "     ## should be replaced with DB QUERY ## factorization could be applied to save storage ## some rollback mechanism should implemented on DB level\n",
    "user_item_from_start.to_csv(output_folder+'CF_click/today/user_item_from_start.csv') \n",
    "pd.read_csv(output_folder+'CF_click/today/user_item_from_start.csv').to_csv(output_folder+'CF_click/predate/user_item_from_start.csv') # copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.254302Z",
     "start_time": "2021-04-20T22:58:34.999Z"
    }
   },
   "outputs": [],
   "source": [
    "## fake initialization to a time t0\n",
    "rec_from_start = load_daily_data(now= \"2018-03-13\", date_field='order_date', file_path=PATH_ORDER, from_start = True)\n",
    "# user_item_from_start = generate_user_item_order_matrix(rec_from_start)\n",
    "\n",
    "## generate daily \n",
    "# rec_daily = load_daily_data(now= \"2018-03-13\", date_field='order_date', file_path=PATH_ORDER)\n",
    "# user_item_daily = generate_user_item_matrix(rec_daily, based='order')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily offline process - server side\n",
    "- DB version should be used to replace this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load click and user table \n",
    "- Time attrs added\n",
    "- label encoding for \"sku_ID\", \"user_ID\", \"order_ID\" and perserve the original one for final evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.255303Z",
     "start_time": "2021-04-20T22:58:35.125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_click_order(click_cols = ['user_ID', 'sku_ID',  'day', 'month', 'year', 'hour','request_time_sec'],\n",
    "                     order_cols = ['user_ID', 'sku_ID',  'day', 'month', 'year', 'hour', 'order_ID', 'order_time_sec'],\n",
    "                     sku_cols = ['sku_ID', 'type', 'brand_ID'],\n",
    "                     sort = ['user_ID', 'request_time_sec'], num_samples=None, need_encode=True, file_version='test'):\n",
    "    '''\n",
    "    \n",
    "    1. 读取4个表\n",
    "    2. 添加时间属性3列 (day month year) etc\n",
    "    3. label encoding (user_ID, sku_ID). origin ID 保存在user_table 和sku_table\n",
    "    4. outer join table (因为暂时还不确定是在一天内. 如果确定在一天内 request链接 并下单, 则用left join)\n",
    "        - 例如:  左侧为空, 右侧有的, 就是可能在前些天 有request, 然后过了几天才下单. 这个问题需要通过 request_time_sec 来做差解决\n",
    "    5. 返回df (做EDA的数据已经另存, 此处不需要返回 order_table 和 click_table)\n",
    "    \n",
    "    click_table.columns = ['sku_ID', 'user_ID', 'request_time', 'channel', 'request_time_sec',\n",
    "           'hour', 'day', 'month', 'year', 'daysinmonth', 'dayofyear',\n",
    "           'request_date']     \n",
    "\n",
    "    order_table.columns = ['order_ID', 'user_ID', 'sku_ID', 'order_date', 'order_time', 'quantity',\n",
    "           'type', 'promise', 'original_unit_price', 'final_unit_price',\n",
    "           'direct_discount_per_unit', 'quantity_discount_per_unit',\n",
    "           'bundle_discount_per_unit', 'coupon_discount_per_unit', 'gift_item',\n",
    "           'dc_ori', 'dc_des', 'order_time_sec', 'hour', 'day', 'month', 'year',\n",
    "           'daysinmonth', 'dayofyear']\n",
    "    \n",
    "    sku_table.columns = ['sku_ID', 'type', 'brand_ID', 'attribute1', 'attribute2',\n",
    "           'activate_date', 'deactivate_date', 'origin_sku_ID']\n",
    "    '''\n",
    "#     click_table = load_dataset.load_click(sort=None)[cols1]\n",
    "#     order_table = load_dataset.load_order()[cols2]\n",
    "#     click_table = load_click(sort=None, num_samples=num_samples)[click_cols1] # 已去除 \"-\" 用户\n",
    "#     order_table = load_order(num_samples=num_samples)[order_cols2]\n",
    "\n",
    "\n",
    "\n",
    "    click_table = load_click(sort=None, num_samples=num_samples)[click_cols] # 已去除 \"-\" 用户\n",
    "    order_table = load_order(num_samples=num_samples)[order_cols]\n",
    "    if (need_encode== False): # 先转换label encoding, 再join效率高\n",
    "        \n",
    "            # 这一步可以在数据库内完成, 而且可以连接 a.day = b.day-1\n",
    "        df = click_table.merge(order_table, how='left',\n",
    "                          left_on = ['user_ID', 'sku_ID', 'day', 'month', 'year'],\n",
    "                          right_on = ['user_ID', 'sku_ID', 'day', 'month', 'year']) # 这里应该是left, 找到所有同一天 点击+下单 的 用户+sku+时间\n",
    "\n",
    "        df['if_order'] =  1*(~df.order_ID.isnull()) \n",
    "        \n",
    "    elif (need_encode== True):\n",
    "        user_table = pd.read_csv(PATH_USER, nrows=num_samples)\n",
    "        sku_table = pd.read_csv(PATH_SKU, nrows=num_samples)\n",
    "        \n",
    "        ## fit_transform \n",
    "        sku_le = preprocessing.LabelEncoder().fit(pd.concat([click_table['sku_ID'],order_table['sku_ID'],sku_table[\"sku_ID\"]], axis=0).astype(str))\n",
    "        click_table['sku_ID'] = sku_le.transform(click_table['sku_ID'])\n",
    "        order_table['sku_ID'] = sku_le.transform(order_table['sku_ID'])\n",
    "        sku_table['origin_sku_ID'] = sku_table['sku_ID'] # 保留原有ID\n",
    "        sku_table['sku_ID'] = sku_le.transform(sku_table['origin_sku_ID']) # 更新 label\n",
    "        \n",
    "        \n",
    "        user_le = preprocessing.LabelEncoder().fit(pd.concat([click_table['user_ID'],order_table['user_ID'], user_table['user_ID']], axis=0).astype(str))\n",
    "        click_table['user_ID'] = user_le.transform(click_table['user_ID'])\n",
    "        order_table['user_ID'] = user_le.transform(order_table['user_ID'])\n",
    "        user_table['origin_user_ID'] = user_table['user_ID'] # 保留原有ID   \n",
    "        user_table['user_ID'] = user_le.transform(user_table['origin_user_ID']) # 更新label\n",
    "        \n",
    "        \n",
    "        order_le = preprocessing.LabelEncoder().fit(order_table['order_ID'].astype(str))\n",
    "        order_table['order_ID'] = order_le.transform(order_table['order_ID'].astype(str))\n",
    "        \n",
    "        # 这一步可以在数据库内完成, 而且可以连接 a.day = b.day-1\n",
    "        df = click_table.merge(order_table, how='outer',\n",
    "                          left_on = ['user_ID', 'sku_ID', 'day', 'month', 'year'],\n",
    "                          right_on = ['user_ID', 'sku_ID', 'day', 'month', 'year']) \n",
    "        \n",
    "        ## Brand_ID\n",
    "        df = df.merge(sku_table[sku_cols],how='left',left_on =['sku_ID'], right_on=['sku_ID'])\n",
    "        \n",
    "        df['if_order'] =  1*(~df.order_ID.isnull()) \n",
    "        \n",
    "        \n",
    "        if file_version != 'test':\n",
    "            df.to_csv(output_folder+\"all_dt_\"+file_version+\".csv\")\n",
    "            user_table.to_csv(output_folder+\"user_table.csv\")\n",
    "            sku_table.to_csv(output_folder+\"sku_table.csv\")\n",
    "    \n",
    "\n",
    "    if sort:\n",
    "        df.sort_values(sort, inplace=True)\n",
    "    return df\n",
    "            \n",
    "\n",
    "\n",
    "# only execute once\n",
    "a= load_click_order(num_samples=None, need_encode=True, file_version='v1')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.256303Z",
     "start_time": "2021-04-20T22:58:35.128Z"
    }
   },
   "outputs": [],
   "source": [
    "output_folder = \"../processed_data/\"\n",
    "num_samples = 100000\n",
    "file_version = \"v1\"\n",
    "df = pd.read_csv(output_folder+'all_dt_'+file_version+'.csv',nrows=num_samples, index_col=0) # click_user_table\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.257303Z",
     "start_time": "2021-04-20T22:58:35.131Z"
    }
   },
   "outputs": [],
   "source": [
    "# a= load_click_order(num_samples=10000000, load=False, file_version='v1') # load generated dataset\n",
    "# a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Combined user features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> Key Assumption: </font>\n",
    "- In this period, the preference of user will not change\n",
    "- The order only happen in the same day as request\n",
    "    - It could be released later if we want to generate more samples\n",
    "- In all the n-grams samples (user request sku list), choose window_size = 5, predict the middle one which is ordered sku_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.369328Z",
     "start_time": "2021-04-20T22:58:35.353325Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't save it before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\py810\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-16dd837b370e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0muser_table_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_encode_user_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0muser_table_encoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;31m# sku_map = pd.read_pickle('sku_map.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# encode user_table\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from pandas import read_csv, datetime, to_datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "import time\n",
    "\n",
    "\n",
    "# encode_user_table 生成\n",
    "def load_encode_user_table(load=True,nrows=None):\n",
    "    if load == False:\n",
    "        user_table = pd.read_csv('user_table.csv', nrows=nrows)\n",
    "        tmp = user_table[['user_level', 'gender','education', 'city_level', 'purchase_power','marital_status','age']].astype(str).progress_apply(lambda x: \"__\".join(list(x)), axis=1)\n",
    "        user_table_encoded = user_table[['user_ID','origin_user_ID']]\n",
    "        user_table_encoded['user_encode'] = tmp\n",
    "        # user_map_dict = user_table_encoded.set_index('user_ID').T.to_dict() # 这个转换过程特别慢. 但是后续合并很快\n",
    "        user_table_encoded.to_pickle('user_table_encoded.pkl')\n",
    "    else:\n",
    "        try:\n",
    "            user_table_encoded = pd.read_pickle('user_table_encoded.pkl')\n",
    "\n",
    "        except:\n",
    "            print(\"Didn't save it before\")\n",
    "            return None\n",
    "    return user_table_encoded   \n",
    "\n",
    "user_table_encoded = load_encode_user_table(load=True)\n",
    "user_table_encoded.head()\n",
    "# sku_map = pd.read_pickle('sku_map.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.371330Z",
     "start_time": "2021-04-20T22:58:35.311Z"
    }
   },
   "outputs": [],
   "source": [
    "# aggregate as dict INFO_Vector \n",
    "# Target structure user_ID:{attrs_combined: xxx__xxx__xxxx , request_list:'xxx__xxxx__xxxx__xxxx__xxxx', orginal_user_id}\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def load_train_dt(load=True, name='dt_train_v2.pkl'):\n",
    "    if load == False:\n",
    "        ### load dataset\n",
    "        user_table_encoded, user_map_dict = load_encode_user_table(load=load,nrows=None)\n",
    "        click_table = pd.read_csv('click_table.csv',usecols= [1,2], nrows=100000000) ## 这个table是我预处理过的table. 已经label encoding 过了\n",
    "        \n",
    "        ###  merge dataset\n",
    "        dt = pd.merge(user_table_encoded, click_table, how='left', on='user_ID')\n",
    "        # dt[~dt['sku_ID'].isnull()]\n",
    "        dt.dropna(how='any', inplace=True) # 是否需要?\n",
    "        dt[['sku_ID']]=dt[['sku_ID']].astype(int) # for convert to string, float type will contain dot zero\n",
    "        \n",
    "        \n",
    "        ### scan the table once and generate INFO_Vector for each user \n",
    "        INFO_Vector = defaultdict(lambda: {'attrs_combined':'','request_list':'', 'orginal_user_id':''})\n",
    "        for i in tqdm(range(len(dt))): \n",
    "            tmprec = {x[0]:x[1] for x in zip(dt.columns,dt.iloc[i])} # 当前行\n",
    "        #     INFO_Vector[tmprec['user_ID']]['attrs_combined']+=str(tmprec['user_encode'])+',' # 当前行信息储存到对应的 user INFO_Vector\n",
    "            INFO_Vector[tmprec['user_ID']]['request_list']+=str(tmprec['sku_ID'])+'__' # 当前行信息储存到对应的 user INFO_Vector\n",
    "        #     INFO_Vector[tmprec['user_ID']]['attrs_combined']+=str(tmprec['user_encode'])+',' # 当前行信息储存到对应的 user INFO_Vector\n",
    "\n",
    "        #     if tmprec['request_time_sec']==tmprec['request_time_sec']: # 判断不是 nan, 则\n",
    "        #         INFO_Vector[tmprec['user_ID']]['ts']+=str(tmprec['request_time_sec'])+','\n",
    "        #     else: # 是nan, 则找order_time\n",
    "        #         INFO_Vector[tmprec['user_ID']]['ts']+=str(tmprec['order_time_sec'])+','\n",
    "        #     INFO_Vector[tmprec['user_ID']]['neg']+=str()+','\n",
    "        # #     INFO_Vector[tmprec['user_ID']]['buy']+=str(tmprec['request_time_sec'])+','\n",
    "        #     INFO_Vector[tmprec['user_ID']]['order']+=str(tmprec['if_order'])+','\n",
    "        \n",
    "        #### update map for INFO_Vector\n",
    "        user_map_dict = user_table_encoded.set_index('user_ID').T.to_dict() # 这个转换过程特别慢. 但是后续合并很快\n",
    "        for i in tqdm(INFO_Vector.keys()):\n",
    "            INFO_Vector[i]['attrs_combined'] = user_map_dict[i]['user_encode']\n",
    "            INFO_Vector[i]['orginal_user_id'] = user_map_dict[i]['origin_user_ID']\n",
    "\n",
    "        dt_train = pd.DataFrame(INFO_Vector).T\n",
    "        dt_train.to_pickle(name) # 保存数据\n",
    "    else:\n",
    "        try:\n",
    "            dt_train = pd.read_pickle(name)\n",
    "        except:\n",
    "            print(\"Didn't save the file with this name before\")\n",
    "    return dt_train\n",
    "\n",
    "## 这是W2V的model数据\n",
    "dt_train = load_train_dt(load=True, name='dt_train.pkl')\n",
    "# dt_train.T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.371330Z",
     "start_time": "2021-04-20T22:58:35.314Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.484355Z",
     "start_time": "2021-04-20T22:58:35.480354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\OneDrive - stevens.edu\\\\Stevens DS\\\\CS609\\\\project\\\\RS_code\\\\preprocessing'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the orginal preprocessing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.499358Z",
     "start_time": "2021-04-20T22:58:35.485355Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rnn_dt_train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8e2ca9aaa324>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# 300MB 的pickle文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rnn_dt_train.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"infer\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# 1) try standard library Pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rnn_dt_train.pkl'"
     ]
    }
   ],
   "source": [
    "# a = load_click_order(num_samples=100000000, load=True, file_version='v1')\n",
    "# from collections import defaultdict\n",
    "# import numpy as np\n",
    "# # INFO_Vector = defaultdict(lambda: {'seq':'', 'ts':'','date':'', 'neg':'', 'buy':'', 'order':'', 'time_gap':'','last_time_request': 0.0})\n",
    "\n",
    "# INFO_Vector = defaultdict(lambda: {'seq':'', 'order':'', 'time_gap':'','last_time_request': 0.0})\n",
    "# for i in tqdm(range(len(a))): # 遍历数据表格一次, 保存所有信息\n",
    "#     tmprec = {x[0]:x[1] for x in zip(a.columns,a.iloc[i])} # 当前行\n",
    "#     INFO_Vector[tmprec['user_ID']]['seq']+=str(tmprec['sku_ID'])+',' # 当前行信息储存到对应的 user INFO_Vector\n",
    "# #     INFO_Vector[tmprec['user_ID']]['date']+=str(tmprec['year'])+'-'+str(tmprec['month'])+'-'+str(tmprec['day'])+','\n",
    "\n",
    "# #     if tmprec['request_time_sec']==tmprec['request_time_sec']: # 判断不是 nan, 则\n",
    "# #         INFO_Vector[tmprec['user_ID']]['ts']+=str(tmprec['request_time_sec'])+','\n",
    "# #     else: # 是nan, 则找order_time\n",
    "# #         INFO_Vector[tmprec['user_ID']]['ts']+=str(tmprec['order_time_sec'])+','\n",
    "# #     INFO_Vector[tmprec['user_ID']]['neg']+=str()+','\n",
    "# # #     INFO_Vector[tmprec['user_ID']]['buy']+=str(tmprec['request_time_sec'])+','\n",
    "#     INFO_Vector[tmprec['user_ID']]['order']+=str(tmprec['if_order'])+','\n",
    "    \n",
    "    \n",
    "#     # 上次时间 减去这次时间 # 会有负值, 因为是join一天, 有可能下单在request之前的错误\n",
    "#     INFO_Vector[tmprec['user_ID']]['time_gap'] += str(INFO_Vector[tmprec['user_ID']]['last_time_request'] -  tmprec['request_time_sec'])+','\n",
    "    \n",
    "#     # 更新时间记录\n",
    "#     INFO_Vector[tmprec['user_ID']]['last_time_request'] = tmprec['request_time_sec']\n",
    "\n",
    "\n",
    "# rnn_dt_train = pd.DataFrame(INFO_Vector).T\n",
    "# rnn_dt_train.to_pickle('rnn_dt_train.pkl') # 保存数据\n",
    "# rnn_dt_train\n",
    "\n",
    "\n",
    "# 300MB 的pickle文件\n",
    "a =  pd.read_pickle('rnn_dt_train.pkl')\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {user_ID: {\"seq\": sku_ID sequence}\n",
    "- with the help of nltk ngram function, generate training sample based on the diction structure data\n",
    "- window_size = 11 means: consider 11 request times. If one of them is ordered, one sample will be generated [context_sku_ID, center_sku_ID]\n",
    "- Detail: In the middle of 11 request time, the middle sku_ID (the six) is order, ten sku_ID around this center will be X, the center will be y \n",
    "    - If there are two order continuously, it means this sequence will generate two samples in window size = 11  \n",
    "    - <font color=red>理解为, 只要他买了一个物品, 周围10个request都是可能的商品, 生成样本的时候, 把其他都看成0</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Models - base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.623386Z",
     "start_time": "2021-04-20T22:58:35.602381Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'myutils_V4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2e9cf7880b2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmyutils_V4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'myutils_V4'"
     ]
    }
   ],
   "source": [
    "from myutils_V4 import *\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Input, Flatten, Concatenate\n",
    "from collections import Counter, defaultdict\n",
    "from gensim.models import word2vec\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from tensorflow.keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import bigrams, trigrams , ngrams\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from sklearn import preprocessing\n",
    "# from tensorflow.keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from IPython.display import SVG\n",
    "from numpy.random import seed\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import gensim.downloader as api\n",
    "import glob\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import string, os \n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# tf.compat.v1.enable_eager_execution(config=None, device_policy=None, execution_mode=None)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # 关掉warning信息\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.624386Z",
     "start_time": "2021-04-20T22:58:35.573Z"
    }
   },
   "outputs": [],
   "source": [
    "# 300MB 的pickle文件\n",
    "rnn_dt_train =  pd.read_pickle('rnn_dt_train.pkl')\n",
    "# num_samples= 10000000\n",
    "\n",
    "\n",
    "num_samples = 100000000\n",
    "INFO_Vector = rnn_dt_train[0:num_samples]\n",
    "corpus = [i.split(',') for i in INFO_Vector['seq']] \n",
    "INFO_Vector = INFO_Vector.T.to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.625387Z",
     "start_time": "2021-04-20T22:58:35.576Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Clustering for user?\n",
    "# processer = TfidfVectorizer(max_df=1.0, min_df=5)    # 至少在5个文档中出现过\n",
    "# tfidf = processer.fit_transform(INFO_Vector['seq'])\n",
    "# aa = tfidf.toarray()\n",
    "# EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:35.627387Z",
     "start_time": "2021-04-20T22:58:35.579Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS_ratio = 0.95\n",
    "MAX_DOC_LEN_ratio = 0.90\n",
    "char_level_switch = False\n",
    "MAX_NB_WORDS = eda_MAX_NB_WORDS(corpus, ratio = MAX_NB_WORDS_ratio, filters=' ',char_level = char_level_switch)\n",
    "MAX_DOC_LEN = eda_MAX_DOC_LEN(corpus, ratio = MAX_DOC_LEN_ratio, filters=' ',char_level = char_level_switch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lots of information are drop\n",
    "- because most of user doesn't have enough request information \n",
    "- with more data, this process will be largely improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.300539Z",
     "start_time": "2021-04-20T22:58:35.713407Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_DOC_LEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3ac20a311c13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_DOC_LEN\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 80%的样本 request list 的长度, 除以2. 生成对应 n-gram样本. 并以中间为1的为一个样本\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuywhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 比这个短的直接没了\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msku_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MAX_DOC_LEN' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "window_size = int(MAX_DOC_LEN/2) # 80%的样本 request list 的长度, 除以2. 生成对应 n-gram样本. 并以中间为1的为一个样本\n",
    "\n",
    "def get_samples(tokens, buywhat, window_size): # 比这个短的直接没了\n",
    "    sku_list = ngrams(tokens, window_size)\n",
    "    order_list = ngrams(buywhat, window_size)\n",
    "    n_grams_sku =[]\n",
    "    for order_grams, sku_grams in zip(order_list, sku_list):\n",
    "        if order_grams[int(window_size/2)]=='1': # 如果中间这个词为1, 那么周围10个单位, 预测中间这个词 会买\n",
    "            X = list(sku_grams)\n",
    "            y = X.pop(int(window_size/2))\n",
    "            n_grams_sku.append([X, y]) # append([(x), y])\n",
    "    return n_grams_sku\n",
    "\n",
    "\n",
    "# get_samples(tokens, buywhat, window_size)\n",
    "\n",
    "ngram_samples = []\n",
    "ignore_set=0\n",
    "for i in list(INFO_Vector):\n",
    "    tokens = INFO_Vector[i]['seq'].split(',')[0:-1] \n",
    "    buywhat =  INFO_Vector[i]['order'].split(',')[0:-1] # 以逗号分隔, 然后去掉最后一个逗号\n",
    "#     print(tokens,buywhat)\n",
    "    tmp_sample = get_samples(tokens, buywhat, window_size)\n",
    "    if len(tmp_sample)>=1: # 如果不为空, 则填入到 training data\n",
    "        ngram_samples.extend(tmp_sample)\n",
    "    else:\n",
    "        ignore_set+=1 # 计数, 丢掉了多少个user信息\n",
    "        \n",
    "print(\"{} user information drop: About ({:.2f}%) \".format(ignore_set, ignore_set/(len(INFO_Vector))*100))\n",
    "\n",
    "docs = pd.DataFrame(ngram_samples,columns=['X','y'])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and genarate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.302539Z",
     "start_time": "2021-04-20T22:58:35.709Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# 用sparse categorical loss 就不用对y进行one hot\n",
    "\n",
    "test_ratio = 0.1\n",
    "seed=2\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(docs['X'],docs['y'],test_size=test_ratio, random_state=seed)\n",
    "processor = text_preprocessor(MAX_DOC_LEN, MAX_NB_WORDS, docs['X'])\n",
    "\n",
    "x_train = processor.generate_seq(x_train)\n",
    "# y_train = to_categorical(y_train)\n",
    "y_train = y_train.astype(int)\n",
    "x_test = processor.generate_seq(x_test)\n",
    "# y_test = to_categorical(y_test)\n",
    "y_test = y_test.astype(int)\n",
    "print('Shape of x_tr: ' + str(x_train.shape))\n",
    "print('Shape of y_tr: ' + str(y_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))\n",
    "\n",
    "output_shape = max(y_train)+1 # 为了满足sparse categorical loss的计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.303540Z",
     "start_time": "2021-04-20T22:58:35.711Z"
    }
   },
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def Best_model_report(grid_result, to_file='GV_result.xlsx' ):\n",
    "    GV_result = pd.DataFrame(grid_result.cv_results_)\n",
    "    GV_result.to_excel(to_file)\n",
    "#     y_pred = grid_result.predict(x_test)\n",
    "#     y_test_one=np.argmax(y_test,axis=1)\n",
    "#     cm = confusion_matrix(y_test_one, y_pred)\n",
    "#     print('confusion matrix:\\n', cm)\n",
    "#     print('classification report:\\n', classification_report(y_test_one, y_pred))\n",
    "    return GV_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.304540Z",
     "start_time": "2021-04-20T22:58:35.714Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, BATCH_SIZE, NUM_EPOCHES, BestModel_Name=\"best_model\", patience=10 ): # Final one step\n",
    "    \n",
    "    #### Best model selection \n",
    "    BEST_MODEL_FILEPATH = BestModel_Name\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='min') # patience: number of epochs with no improvement on monitor : val_loss\n",
    "    # monitoring\n",
    "    checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    history = model.fit(x_train, y_train, validation_split=0.2, batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, callbacks=[earlyStopping, checkpoint], verbose=2)\n",
    "    model.load_weights(BestModel_Name)\n",
    "\n",
    "    #### classification Report\n",
    "    history_plot(history)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred>0.5))\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print( \"\\n\\n\\n\")\n",
    "    return y_pred # 也许能出 tpr 和 fpr图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.305541Z",
     "start_time": "2021-04-20T22:58:35.717Z"
    }
   },
   "outputs": [],
   "source": [
    "# define Model for classification\n",
    "def model_Create(FS, NF, EMB, MDL, MNW, PWV = None, optimizer='RMSprop', trainable_switch=True):\n",
    "    cnn_box = cnn_model(FILTER_SIZES=FS, MAX_NB_WORDS=MNW, MAX_DOC_LEN=MDL, EMBEDDING_DIM=EMB, NUM_FILTERS=NF, PRETRAINED_WORD_VECTOR=PWV, trainable_switch=trainable_switch)\n",
    "    q1_input = Input(shape=(MDL,), dtype='int32', name='q1_input') # Hyperparameters: MAX_DOC_LEN\n",
    "    encode_input1 = cnn_box(q1_input)\n",
    "    half_features = int(len(FS)*NF/2)\n",
    "    dense1 = Dense(half_features,activation='relu', name='half_features')(encode_input1)\n",
    "    drop_1 = Dropout(rate=0.4, name='dropout')(dense1)\n",
    "    pred = Dense(output_shape,activation='softmax', name='Prediction')(drop_1)\n",
    "    \n",
    "    model = Model(inputs=q1_input, outputs=pred)    \n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# model = model_Create(FS=[2,3,4], NF=12, EMB=200, MDL=19, MNW=2126, PWV = CBOW_W2V,trainable_switch=False)\n",
    "# model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T17:19:01.170629Z",
     "start_time": "2020-05-12T17:19:01.166629Z"
    }
   },
   "source": [
    "## Explaination \n",
    "- When predicting, based on user request id sequence, recommend the order sku_id generated by models\n",
    "- <font color=red> Only need to review the score of the evaluation result for each RNNs</font> The classification report is the CNNs result. I forget to skip it.\n",
    "- CNNs: 92.80%\n",
    "- RNNs: 0.807, 0.77538645, 0.7691645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained CBOW_W2V for sku_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.306541Z",
     "start_time": "2021-04-20T22:58:35.854Z"
    }
   },
   "outputs": [],
   "source": [
    "EMB = [100]\n",
    "iter_step= 300\n",
    "CBOW_W2V =  processor.w2v_pretrain(EMB[0], min_count=2, seed=1, cbow_mean=1,negative=5, window=5, iter=iter_step, workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs\n",
    "- the parameters can be editted into grid search version\n",
    "- But there is a bug need to be handle later\n",
    "    - Pretrained embedding cannot be fixed in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.307541Z",
     "start_time": "2021-04-20T22:58:35.924Z"
    }
   },
   "outputs": [],
   "source": [
    "patience = 10\n",
    "epoch = 30\n",
    "n_jobs = 1 # if use GPU, this have to be one.\n",
    "\n",
    "file_name = 'test'\n",
    "BestModel_Name = file_name+ 'Best_GS'\n",
    "\n",
    "############# Set hyper parameters\n",
    "FILTER_SIZES= [4,5,6,7,8]\n",
    "NUM_FILTERS=24\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE=128 # increase speed with large batch size and avoid overfit or wrong direction\n",
    "NUM_EPOCHES=20 # patience=20\n",
    "# CBOW_W2V = processor.w2v_pretrain(EMBEDDING_DIM) # 需要train, 比较慢\n",
    "# Glove_W2V = processor.load_glove_w2v(EMBEDDING_DIM) # 需要下载, 比较慢\n",
    "OPT = optimizers.Adam(lr=1e-4)\n",
    "trainable_switch=False\n",
    "\n",
    "\n",
    "model = model_Create(FS=FILTER_SIZES, NF=NUM_FILTERS, MDL=MAX_DOC_LEN,MNW=MAX_NB_WORDS+1, EMB=EMBEDDING_DIM, PWV = CBOW_W2V, trainable_switch=trainable_switch, optimizer=OPT )\n",
    "# model_best_1_pred = train_model(model, x_train, y_train, x_test, y_test, BATCH_SIZE, NUM_EPOCHES, BestModel_Name=BestModel_Name)\n",
    "# model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.307541Z",
     "start_time": "2021-04-20T22:58:35.927Z"
    }
   },
   "outputs": [],
   "source": [
    "BEST_MODEL_FILEPATH = BestModel_Name\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='min') # patience: number of epochs with no improvement on monitor : val_loss\n",
    "# monitoring\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "history = model.fit(x_train, y_train, validation_split=0.2, batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, callbacks=[earlyStopping, checkpoint], verbose=2)\n",
    "model.load_weights(BestModel_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.308541Z",
     "start_time": "2021-04-20T22:58:35.930Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### classification Report\n",
    "history_plot(history)\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T21:23:58.614653Z",
     "start_time": "2020-05-11T21:23:38.196733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T21:23:58.948740Z",
     "start_time": "2020-05-11T21:23:58.615652Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.310541Z",
     "start_time": "2021-04-20T22:58:36.016Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# 用sparse categorical loss 就不用对y进行one hot\n",
    "\n",
    "test_ratio = 0.1\n",
    "seed=2\n",
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(docs['X'],docs['y'],test_size=test_ratio, random_state=seed)\n",
    "processor = text_preprocessor(MAX_DOC_LEN, MAX_NB_WORDS, docs['X'])\n",
    "\n",
    "x_train = processor.generate_seq(x_train)\n",
    "# y_train = to_categorical(y_train)\n",
    "y_train = y_train.astype(int)\n",
    "x_test = processor.generate_seq(x_test)\n",
    "# y_test = to_categorical(y_test)\n",
    "y_test = y_test.astype(int)\n",
    "print('Shape of x_tr: ' + str(x_train.shape))\n",
    "print('Shape of y_tr: ' + str(y_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))\n",
    "\n",
    "output_shape = max(y_train)+1 # 为了满足sparse categorical loss的计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.311541Z",
     "start_time": "2021-04-20T22:58:36.019Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"MAX_DOC_LEN\", MAX_DOC_LEN)\n",
    "print(\"MAX_NB_WORDS\", MAX_NB_WORDS)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.311541Z",
     "start_time": "2021-04-20T22:58:36.022Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.312541Z",
     "start_time": "2021-04-20T22:58:36.025Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "latent_dim = 32\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "### construct the RNN with GRU unit\n",
    "model_0 = Sequential()\n",
    "model_0.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM)) #  embedding dimension , 这里的输入应该是 one-hot 的19个词. 还是直接sequence. 都行\n",
    "model_0.add(LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.5,return_sequences=True))\n",
    "model_0.add(LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.5,return_sequences=True))\n",
    "model_0.add(LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.5,return_sequences=False))\n",
    "model_0.add(Dropout(0.4))\n",
    "model_0.add(Dense(output_shape, activation='softmax')) # 因为 y 是经过 one-hot 的, 所以他能保存位置信息.\n",
    "\n",
    "model_0.summary()\n",
    "model_0.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "history = model_0.fit(x_train, y_train, epochs=10,  batch_size=128, validation_split=0.1,  shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.313542Z",
     "start_time": "2021-04-20T22:58:36.028Z"
    }
   },
   "outputs": [],
   "source": [
    "#### classification Report\n",
    "history_plot(history)\n",
    "# y_pred = model.predict(x_test) # 内存不够\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model_0.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T00:58:13.526929Z",
     "start_time": "2020-05-07T00:58:13.523927Z"
    }
   },
   "source": [
    "### Inverse encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.314541Z",
     "start_time": "2021-04-20T22:58:36.113Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RNN_recommend_result =[] \n",
    "\n",
    "for seq in tqdm(x_test):\n",
    "    sku_record = []\n",
    "    for i in seq:\n",
    "        sku_record.append(processor.index_word[i]) # 返回商品顺序\n",
    "    idx = np.argmax(model.predict(seq.reshape(1,-1)),axis=1)[0] # 找到最大概率的商品\n",
    "    sku_record.append(idx)\n",
    "    RNN_recommend_result.append(sku_record)\n",
    "    \n",
    "RNN_recommend_result = pd.DataFrame(RNN_recommend_result)\n",
    "RNN_recommend_result.columns = list(RNN_recommend_result.columns[0:-1])+['recommend']\n",
    "RNN_recommend_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.316545Z",
     "start_time": "2021-04-20T22:58:36.199Z"
    }
   },
   "outputs": [],
   "source": [
    "### construct the RNN with GRU unit\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM)) #  embedding dimension , 这里的输入应该是 one-hot 的19个词. 还是直接sequence. 都行\n",
    "model_1.add(Bidirectional(LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.2,return_sequences=False)))\n",
    "model_1.add(Dropout(0.4))\n",
    "model_1.add(Dense(output_shape, activation='softmax')) # 因为 y 是经过 one-hot 的, 所以他能保存位置信息.\n",
    "\n",
    "model_1.summary()\n",
    "model_1.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "history = model_1.fit(x_train, y_train, epochs=10,  batch_size=128, validation_split=0.1,  shuffle=True) \n",
    "\n",
    "\n",
    "#### classification Report\n",
    "history_plot(history)\n",
    "# y_pred = model_1.predict(x_test) # 内存不够\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model_1.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model_1.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.317543Z",
     "start_time": "2021-04-20T22:58:36.202Z"
    }
   },
   "outputs": [],
   "source": [
    "### construct the RNN with GRU unit\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM)) #  embedding dimension , 这里的输入应该是 one-hot 的19个词. 还是直接sequence. 都行\n",
    "model_2.add((LSTM(latent_dim, dropout=0.0, recurrent_dropout=0.2,return_sequences=False)))\n",
    "model_2.add(Dropout(0.4))\n",
    "model_2.add(Dense(output_shape, activation='softmax')) # 因为 y 是经过 one-hot 的, 所以他能保存位置信息.\n",
    "\n",
    "model_2.summary()\n",
    "model_2.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "history = model_2.fit(x_train, y_train, epochs=10,  batch_size=128, validation_split=0.1,  shuffle=True) \n",
    "\n",
    "#### classification Report\n",
    "history_plot(history)\n",
    "# y_pred = model_2.predict(x_test) # 内存不够\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model_2.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model_2.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.318543Z",
     "start_time": "2021-04-20T22:58:36.205Z"
    }
   },
   "outputs": [],
   "source": [
    "### construct the RNN with GRU unit\n",
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM)) #  embedding dimension , 这里的输入应该是 one-hot 的19个词. 还是直接sequence. 都行\n",
    "model_3.add((GRU(latent_dim, dropout=0.0, recurrent_dropout=0.2,return_sequences=False)))\n",
    "model_3.add(Dropout(0.4))\n",
    "model_3.add(Dense(output_shape, activation='softmax')) # 因为 y 是经过 one-hot 的, 所以他能保存位置信息.\n",
    "\n",
    "model_3.summary()\n",
    "model_3.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "history = model_3.fit(x_train, y_train, epochs=10,  batch_size=128, validation_split=0.1,  shuffle=True) \n",
    "\n",
    "\n",
    "\n",
    "#### classification Report\n",
    "history_plot(history)\n",
    "# y_pred = model_3.predict(x_test) # 内存不够\n",
    "# print(classification_report(y_test, np.argmax(y_pred, axis=1)))\n",
    "scores = model_3.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model_3.metrics_names[1], scores[1]*100))\n",
    "print( \"\\n\\n\\n\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T05:32:45.939723Z",
     "start_time": "2020-05-06T05:32:45.935139Z"
    }
   },
   "source": [
    "# Other EDA - Tableau could handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:23:45.268077Z",
     "start_time": "2020-05-06T17:23:45.264077Z"
    }
   },
   "source": [
    "## Pie - platform distributionm\n",
    "- Color map: https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.417566Z",
     "start_time": "2021-04-20T22:58:36.414565Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_CLICK = './data/JD_click_data.csv'\n",
    "PATH_USER = './data/JD_user_data.csv'\n",
    "PATH_SKU = './data/JD_sku_data.csv'\n",
    "PATH_ORDER = './data/JD_order_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.454573Z",
     "start_time": "2021-04-20T22:58:36.419566Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File order_table.csv does not exist: 'order_table.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9bba901ef828>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0morder_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'order_table.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# order_table.to_csv('order_table_tmp.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# order_table.to_csv('order_table.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py810\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File order_table.csv does not exist: 'order_table.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "order_table = pd.read_csv('order_table.csv',nrows=100000)\n",
    "# order_table.to_csv('order_table_tmp.csv')\n",
    "# order_table.to_csv('order_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.456574Z",
     "start_time": "2021-04-20T22:58:36.371Z"
    }
   },
   "outputs": [],
   "source": [
    "click_table = pd.read_csv('click_table.csv',nrows=100000)\n",
    "# click_table.to_csv('click_table_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.457574Z",
     "start_time": "2021-04-20T22:58:36.374Z"
    }
   },
   "outputs": [],
   "source": [
    "click_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T22:58:36.458575Z",
     "start_time": "2021-04-20T22:58:36.377Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_3 = click_table.groupby(['channel']).count()\n",
    "df_3.iloc[:,0].plot(\n",
    "    kind='pie',\n",
    "    table=df_3.iloc[:,1],\n",
    "    autopct='%1.1f%%', cmap='Set3',figsize=(8,8)\n",
    ")\n",
    "\n",
    "# View the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "347.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
